<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Voice Assistant - Chatterbox TTS</title>
    <link rel="stylesheet" href="css/style.css">
    <style>
        .chatterbox-controls {
            background: rgba(0, 0, 0, 0.1);
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            border: 2px solid #4CAF50;
        }
        .chatterbox-title {
            color: #4CAF50;
            font-weight: bold;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
        }
        .voice-controls {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 15px;
            margin-bottom: 15px;
        }
        .control-group {
            display: flex;
            flex-direction: column;
        }
        .control-group label {
            margin-bottom: 5px;
            font-weight: bold;
            color: #333;
        }
        .slider-container {
            display: flex;
            align-items: center;
            gap: 10px;
        }
        .slider {
            flex: 1;
            height: 6px;
            border-radius: 3px;
            background: #ddd;
            outline: none;
        }
        .slider::-webkit-slider-thumb {
            appearance: none;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: #4CAF50;
            cursor: pointer;
        }
        .status-indicator {
            display: inline-block;
            width: 10px;
            height: 10px;
            border-radius: 50%;
            margin-right: 8px;
            background: #4CAF50;
            animation: pulse 2s infinite;
        }
        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.5; }
            100% { opacity: 1; }
        }
        .voice-preview {
            background: #f5f5f5;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-top: 10px;
            font-style: italic;
            color: #666;
        }
        .recording-active {
            animation: recordingPulse 1s infinite;
        }
        @keyframes recordingPulse {
            0% { background-color: #f44336; }
            50% { background-color: #d32f2f; }
            100% { background-color: #f44336; }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
                    <h1>ŒõLFRED</h1>
        <p>Autonomous Linguistic Framework for Retrieval & Enhanced Dialogue</p>
        </header>
        
        <main>
            <!-- Voice Engine Controls -->
            <div class="chatterbox-controls">
                <h3 class="chatterbox-title">
                    <span class="status-indicator"></span>
                    <span id="engine-title">Voice Engine</span>
                </h3>
                
                <div class="voice-controls">
                    <div class="control-group">
                        <label for="voice-engine">Voice Engine:</label>
                        <select id="voice-engine" onchange="switchEngine()">
                            <option value="chatterbox" selected>Chatterbox TTS (High Quality + Voice Cloning)</option>
                            <option value="maya1">Maya1 TTS (3B Transformer + Expressive + GPU)</option>
                            <option value="edge_tts">Edge TTS (Fast + Many Voices)</option>
                            <option value="elevenlabs">ElevenLabs (Premium + Ultra Fast)</option>
                        </select>
                    </div>
                    
                    <!-- Dynamic voice selection based on engine -->
                    <div class="control-group" id="voice-selection-group">
                        <label for="voice-select" id="voice-select-label">Voice:</label>
                        <select id="voice-select" onchange="updateVoiceSelection()">
                            <option value="loading">Loading voices...</option>
                        </select>
                    </div>
                    
                    <div class="control-group">
                        <label>
                            <input type="checkbox" id="tts-enabled" checked onchange="toggleTTS()">
                            <span id="tts-enable-label">Enable Voice Synthesis</span>
                        </label>
                    </div>
                    
                    <!-- Chatterbox-specific controls -->
                    <div id="chatterbox-controls" style="display: block;">
                        <div class="control-group">
                            <label for="emotion-slider">Emotion Level: <span id="emotion-value">70%</span></label>
                            <div class="slider-container">
                                <span>üòê</span>
                                <input type="range" id="emotion-slider" class="slider" min="0" max="100" value="70" oninput="updateEmotion(this.value)">
                                <span>üé≠</span>
                            </div>
                        </div>
                        
                        <div class="control-group">
                            <label for="pacing-slider">Speech Pacing: <span id="pacing-value">40%</span></label>
                            <div class="slider-container">
                                <span>üêå</span>
                                <input type="range" id="pacing-slider" class="slider" min="30" max="70" value="40" oninput="updatePacing(this.value)">
                                <span>üöÄ</span>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Edge TTS-specific controls -->
                    <div id="edge-controls" style="display: none;">
                        <div class="control-group">
                            <label for="edge-speed">Speech Speed: <span id="speed-value">100%</span></label>
                            <div class="slider-container">
                                <span>üêå</span>
                                <input type="range" id="edge-speed" class="slider" min="50" max="200" value="100" oninput="updateSpeed(this.value)">
                                <span>üöÄ</span>
                            </div>
                        </div>
                        
                        <div class="control-group">
                            <label for="edge-pitch">Pitch: <span id="pitch-value">0Hz</span></label>
                            <div class="slider-container">
                                <span>üîΩ</span>
                                <input type="range" id="edge-pitch" class="slider" min="-10" max="10" value="0" oninput="updatePitch(this.value)">
                                <span>üîº</span>
                            </div>
                        </div>
                        
                        <div class="control-group">
                            <label for="edge-volume">Volume: <span id="volume-value">0%</span></label>
                            <div class="slider-container">
                                <span>üîâ</span>
                                <input type="range" id="edge-volume" class="slider" min="-20" max="20" value="0" oninput="updateVolume(this.value)">
                                <span>üîä</span>
                            </div>
                        </div>
                    </div>
                    
                    <!-- ElevenLabs-specific controls -->
                    <div id="elevenlabs-controls" style="display: none;">
                        <div class="control-group">
                            <label for="elevenlabs-stability">Stability: <span id="stability-value">50%</span></label>
                            <div class="slider-container">
                                <span>üéØ</span>
                                <input type="range" id="elevenlabs-stability" class="slider" min="0" max="100" value="50" oninput="updateStability(this.value)">
                                <span>üèîÔ∏è</span>
                            </div>
                        </div>
                        
                        <div class="control-group">
                            <label for="elevenlabs-similarity">Similarity Boost: <span id="similarity-value">75%</span></label>
                            <div class="slider-container">
                                <span>üîÄ</span>
                                <input type="range" id="elevenlabs-similarity" class="slider" min="0" max="100" value="75" oninput="updateSimilarity(this.value)">
                                <span>üé≠</span>
                            </div>
                        </div>
                        
                        <div class="control-group">
                            <label for="elevenlabs-style">Style Intensity: <span id="style-value">0%</span></label>
                            <div class="slider-container">
                                <span>üìñ</span>
                                <input type="range" id="elevenlabs-style" class="slider" min="0" max="100" value="0" oninput="updateStyle(this.value)">
                                <span>üé®</span>
                            </div>
                        </div>
                        
                        <div class="control-group">
                            <label for="elevenlabs-model">Model:</label>
                            <select id="elevenlabs-model" onchange="updateModel(this.value)">
                                <option value="eleven_turbo_v2" selected>Turbo v2 (Fastest)</option>
                                <option value="eleven_monolingual_v1">Monolingual v1 (Quality)</option>
                                <option value="eleven_multilingual_v2">Multilingual v2 (Multi-lang)</option>
                            </select>
                        </div>
                    </div>
                    
                    <!-- Voice Cloning Section (Chatterbox only) -->
                    <div id="voice-cloning-section" style="display: block;">
                    <div style="margin-top: 15px; border-top: 2px solid #4CAF50; padding-top: 15px;">
                        <h4 style="color: #4CAF50; margin-bottom: 10px;">üé§ Voice Cloning (Zero-Shot)</h4>
                        
                        <div class="control-group">
                            <label>Voice Sample Options:</label>
                            <div style="display: flex; gap: 10px; margin-bottom: 10px;">
                                <button type="button" id="record-sample-btn" class="control-btn" onclick="toggleRecording()" style="flex: 1; padding: 8px;">
                                    <span id="record-btn-text">üéôÔ∏è Record Sample</span>
                                </button>
                                <button type="button" onclick="document.getElementById('voice-sample-upload').click()" class="control-btn" style="flex: 1; padding: 8px;">
                                    üìÅ Upload File
                                </button>
                            </div>
                            <input type="file" id="voice-sample-upload" accept="audio/*" onchange="handleVoiceSampleUpload(this)" style="display: none;">
                            <div id="recording-controls" style="display: none; background: #f9f9f9; padding: 10px; border-radius: 5px; margin-bottom: 10px;">
                                <div style="display: flex; align-items: center; gap: 10px;">
                                    <span id="recording-timer">00:00</span>
                                    <div style="flex: 1; background: #ddd; height: 4px; border-radius: 2px;">
                                        <div id="recording-progress" style="background: #f44336; height: 100%; width: 0%; border-radius: 2px; transition: width 0.1s;"></div>
                                    </div>
                                    <span style="font-size: 12px; color: #666;">10s max</span>
                                </div>
                                <small style="color: #666; display: block; margin-top: 5px;">
                                    üí° <strong>Speak clearly for 3-10 seconds.</strong> Say anything: your name, a sentence, count to ten, or read some text aloud.
                                </small>
                            </div>
                            <small style="color: #666;">
                                Record a 3-10 second voice sample or upload an audio file (WAV/MP3). Content doesn't matter - just speak clearly!
                            </small>
                        </div>
                        
                        <div class="control-group">
                            <label>
                                <input type="checkbox" id="voice-cloning-enabled" onchange="toggleVoiceCloning()">
                                Enable Voice Cloning Mode
                            </label>
                        </div>
                        
                        <div id="voice-clone-status" style="padding: 8px; background: #f0f0f0; border-radius: 5px; font-size: 12px; color: #666;">
                            No voice sample uploaded. Using default Chatterbox voices.
                        </div>
                    </div>
                    </div> <!-- End voice-cloning-section -->
                </div>
                
                <div class="voice-preview" id="voice-preview">
                    Current: Female expressive voice with 70% emotion and 30% pacing speed. Engaging female voice with natural expressiveness. ‚ú® Optimized for engaging, natural speech.
                </div>
            </div>
            
            <!-- Voice Assistant Interface -->
            <div id="voice-assistant-root">
                <div class="waveform-container">
                    <canvas id="waveform"></canvas>
                </div>
                
                <div class="controls">
                    <button id="microphone-btn" class="mic-btn" onclick="toggleMicrophone()">
                        <span class="mic-icon">üéôÔ∏è</span>
                        <span id="mic-status">Click to Start</span>
                    </button>
                    <button id="test-voice-btn" class="control-btn" onclick="testChatterboxVoice()">
                        <span>üéµ Test Voice</span>
                    </button>
                    <button id="test-clone-btn" class="control-btn" onclick="testVoiceCloning()" style="display: none;">
                        <span>üé§ Test Clone</span>
                    </button>
                    <button id="transcript-btn" class="control-btn" onclick="toggleTranscript()">
                        <span>üìÑ Transcript</span>
                    </button>
                    <button id="stop-btn" class="control-btn" onclick="stopAll()">
                        <span>‚èπÔ∏è Stop</span>
                    </button>
                </div>
            </div>
            
            <!-- Status Display -->
            <div id="status-display" style="margin-top: 20px; padding: 10px; background: #f0f0f0; border-radius: 5px;">
                <strong>Status:</strong> <span id="current-status">Chatterbox TTS Ready</span>
            </div>
        </main>
        
        <div id="transcript-panel" class="transcript-panel hidden">
            <div class="transcript-header">
                <h2>Conversation Transcript</h2>
                <button id="close-transcript" class="close-btn" onclick="toggleTranscript()">‚úï</button>
            </div>
            <div id="transcript-content" class="transcript-content"></div>
        </div>
    </div>

    <script>
        // Voice Engine Configuration
        const voiceConfig = {
            enabled: true,
            selectedEngine: 'chatterbox',
            baseUrl: window.location.origin,
            // Chatterbox-specific settings
            chatterbox: {
                selectedVoice: 'female_expressive',
                exaggeration: 0.7,       // 70% emotion for engaging speech
                cfg_weight: 0.3,         // Lower weight for better pacing with expressive tone
                voiceCloning: {
                    enabled: false,
                    audioFile: null,
                    fileName: null
                },
                recording: {
                    isRecording: false,
                    mediaRecorder: null,
                    audioChunks: [],
                    startTime: null,
                    maxDuration: 10000, // 10 seconds
                    timer: null
                }
            },
            // Edge TTS-specific settings
            edge: {
                selectedVoice: 'en-US-AriaNeural',
                speed: 1.0,              // Normal speed
                pitch: 0,                // No pitch adjustment
                volume: 0                // No volume adjustment
            },
            // ElevenLabs-specific settings
            elevenlabs: {
                selectedVoice: 'pNInz6obpgDQGcFmaJgB',  // Adam voice ID
                model_id: 'eleven_turbo_v2',           // Fast turbo model
                stability: 0.5,                        // Balanced stability
                similarity_boost: 0.75,                // Good similarity
                style: 0.0,                           // Neutral style
                use_speaker_boost: true               // Enhanced clarity
            }
        };
        
        // Available voices for each engine (loaded dynamically)
        let availableVoices = {
            chatterbox: [],
            edge_tts: [],
            elevenlabs: [
                { id: 'pNInz6obpgDQGcFmaJgB', name: 'Adam (Male, Deep)', gender: 'male', engine: 'elevenlabs' },
                { id: 'EXAVITQu4vr4xnSDxMaL', name: 'Bella (Female, Sweet)', gender: 'female', engine: 'elevenlabs' },
                { id: 'VR6AewLTigWG4xSOukaG', name: 'Arnold (Male, Crisp)', gender: 'male', engine: 'elevenlabs' },
                { id: 'pMsXgVXv3BLzUgSXRplE', name: 'Freya (Female, Conversational)', gender: 'female', engine: 'elevenlabs' },
                { id: 'onwK4e9ZLuTAKqWW03F9', name: 'Daniel (Male, British)', gender: 'male', engine: 'elevenlabs' },
                { id: 'CYw3kZ02Hs0563khs1Fj', name: 'Gigi (Female, Childlike)', gender: 'female', engine: 'elevenlabs' },
                { id: 'bVMeCyTHy58xNoL34h3p', name: 'Jeremy (Male, American)', gender: 'male', engine: 'elevenlabs' },
                { id: 'XB0fDUnXU5powFXDhCwa', name: 'Charlotte (Female, Seductive)', gender: 'female', engine: 'elevenlabs' },
                { id: 'JBFqnCBsd6RMkjVDRZzb', name: 'George (Male, Raspy)', gender: 'male', engine: 'elevenlabs' },
                { id: 'oWAxZDx7w5VEj9dCyTzz', name: 'Grace (Female, Calm)', gender: 'female', engine: 'elevenlabs' }
            ],
            kokoro: [],
            silero: [],
            gtts: [],
            espeak: []
        };

        // Voice configuration matrix: Gender + Emotion combinations
        const voiceMatrix = {
            'female': {
                'expressive': { exaggeration: 0.7, cfg_weight: 0.3, description: 'Engaging female voice with natural expressiveness' },
                'calm': { exaggeration: 0.2, cfg_weight: 0.6, description: 'Soothing female voice with measured pacing' },
                'dramatic': { exaggeration: 0.9, cfg_weight: 0.2, description: 'Intense female voice with maximum emotion' },
                'neutral': { exaggeration: 0.5, cfg_weight: 0.5, description: 'Balanced female voice for general use' }
            },
            'male': {
                'expressive': { exaggeration: 0.6, cfg_weight: 0.3, description: 'Dynamic male voice with controlled emotion' },
                'calm': { exaggeration: 0.3, cfg_weight: 0.6, description: 'Deep, calming male voice with slow delivery' },
                'dramatic': { exaggeration: 0.8, cfg_weight: 0.2, description: 'Powerful male voice with commanding presence' },
                'neutral': { exaggeration: 0.4, cfg_weight: 0.5, description: 'Professional male voice for clear communication' }
            }
        };

        let isListening = false;
        let currentAudio = null;
        let mediaRecorder = null;
        let audioChunks = [];
        let conversationHistory = [];
        
        // Debug conversation history
        console.log('üßπ Initial conversation history:', conversationHistory);

        // Initialize Voice Assistant on page load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('üé≠ Initializing Voice Assistant...');
            
            // Clear any cached conversation history that might contain "No response generated"
            if (conversationHistory.length > 0) {
                console.log('üßπ Clearing conversation history. Previous length:', conversationHistory.length);
                console.log('üßπ Previous history:', conversationHistory);
                conversationHistory = [];
                console.log('üßπ Conversation history cleared');
            }
            loadAvailableVoices();
            checkVoiceStatus();
        });
        
        // Switch between voice engines
        async function switchEngine() {
            const selectedEngine = document.getElementById('voice-engine').value;
            voiceConfig.selectedEngine = selectedEngine;
            
            console.log(`üîÑ Switching to engine: ${selectedEngine}`);
            
            // Hide all engine-specific controls first
            document.getElementById('chatterbox-controls').style.display = 'none';
            document.getElementById('edge-controls').style.display = 'none';
            document.getElementById('elevenlabs-controls').style.display = 'none';
            document.getElementById('voice-cloning-section').style.display = 'none';
            
            // Update UI based on selected engine
            const engineTitles = {
                'chatterbox': 'Chatterbox TTS (High Quality)',
                'maya1': 'Maya1 TTS (3B Transformer + GPU)',
                'kokoro': 'Kokoro TTS (Neural Synthesis)',
                'edge_tts': 'Edge TTS (Fast Response)', 
                'gtts': 'Google TTS (Cloud)',
                'espeak': 'eSpeak TTS (Offline)',
                'elevenlabs': 'ElevenLabs (Premium)'
            };
            
            const engineLabels = {
                'chatterbox': 'Enable Chatterbox TTS',
                'maya1': 'Enable Maya1 TTS',
                'kokoro': 'Enable Kokoro TTS',
                'edge_tts': 'Enable Edge TTS',
                'gtts': 'Enable Google TTS', 
                'espeak': 'Enable eSpeak TTS',
                'elevenlabs': 'Enable ElevenLabs TTS'
            };
            
            // Set title and label
            document.getElementById('engine-title').textContent = engineTitles[selectedEngine] || `${selectedEngine} TTS`;
            document.getElementById('tts-enable-label').textContent = engineLabels[selectedEngine] || `Enable ${selectedEngine} TTS`;
            
            // Show appropriate controls based on engine
            if (selectedEngine === 'chatterbox') {
                document.getElementById('chatterbox-controls').style.display = 'block';
                document.getElementById('voice-cloning-section').style.display = 'block';
            } else if (selectedEngine === 'maya1') {
                // Maya1 uses chatterbox-style controls (supports emotion/expression)
                document.getElementById('chatterbox-controls').style.display = 'block';
                // Voice cloning not supported for Maya1
            } else if (selectedEngine === 'edge_tts') {
                document.getElementById('edge-controls').style.display = 'block';
            } else if (selectedEngine === 'elevenlabs') {
                document.getElementById('elevenlabs-controls').style.display = 'block';
            } else if (selectedEngine === 'kokoro' || selectedEngine === 'silero' || selectedEngine === 'gtts' || selectedEngine === 'espeak') {
                // These engines use basic controls - show chatterbox controls but hide voice cloning
                document.getElementById('chatterbox-controls').style.display = 'block';
                // Voice cloning section already hidden above

            }
            
            // Load voices for selected engine
            await populateVoiceDropdown(selectedEngine);
            updateVoicePreview();
            updateStatus(`üîÑ Switched to ${engineTitles[selectedEngine] || selectedEngine}`);

        }
        
        // Load available voices from API
        async function loadAvailableVoices() {
            try {
                // Load all voices from the single voices endpoint
                const response = await fetch('/api/alfred/voices');
                if (response.ok) {
                    const data = await response.json();
                    const voices = data.voices || [];
                    
                    // Separate voices by engine (case-insensitive)
                    availableVoices.chatterbox = voices.filter(v => 
                        v.engine && (v.engine.toLowerCase() === 'chatterbox' || v.engine.toLowerCase() === 'chatterbox tts')
                    );
                    availableVoices.edge_tts = voices.filter(v => 
                        v.engine && (v.engine.toLowerCase() === 'edge tts' || v.engine.toLowerCase() === 'edge_tts')
                    );
                    
                    // Add ElevenLabs voices (hardcoded popular ones)
                    availableVoices.elevenlabs = [
                        { id: 'pNInz6obpgDQGcFmaJgB', name: 'Adam (Male, Deep)', gender: 'male', engine: 'elevenlabs' },
                        { id: 'EXAVITQu4vr4xnSDxMaL', name: 'Bella (Female, Sweet)', gender: 'female', engine: 'elevenlabs' },
                        { id: 'VR6AewLTigWG4xSOukaG', name: 'Arnold (Male, Crisp)', gender: 'male', engine: 'elevenlabs' },
                        { id: 'pMsXgVXv3BLzUgSXRplE', name: 'Freya (Female, Conversational)', gender: 'female', engine: 'elevenlabs' },
                        { id: 'onwK4e9ZLuTAKqWW03F9', name: 'Daniel (Male, British)', gender: 'male', engine: 'elevenlabs' },
                        { id: 'CYw3kZ02Hs0563khs1Fj', name: 'Gigi (Female, Childlike)', gender: 'female', engine: 'elevenlabs' },
                        { id: 'bVMeCyTHy58xNoL34h3p', name: 'Jeremy (Male, American)', gender: 'male', engine: 'elevenlabs' },
                        { id: 'XB0fDUnXU5powFXDhCwa', name: 'Charlotte (Female, Seductive)', gender: 'female', engine: 'elevenlabs' },
                        { id: 'JBFqnCBsd6RMkjVDRZzb', name: 'George (Male, Raspy)', gender: 'male', engine: 'elevenlabs' },
                        { id: 'oWAxZDx7w5VEj9dCyTzz', name: 'Grace (Female, Calm)', gender: 'female', engine: 'elevenlabs' }
                    ];
                    
                    console.log(`üéµ Loaded ${availableVoices.chatterbox.length} Chatterbox voices, ${availableVoices.edge_tts.length} Edge TTS voices, and ${availableVoices.elevenlabs.length} ElevenLabs voices`);
                    
                    // Populate dropdown for default engine
                    await populateVoiceDropdown(voiceConfig.selectedEngine);
                } else {
                    console.error('Failed to load voices:', response.status, await response.text());
                    updateStatus('‚ö†Ô∏è Failed to load voice options');
                }
                
            } catch (error) {
                console.error('Failed to load voices:', error);
                updateStatus('‚ö†Ô∏è Failed to load voice options');
            }
        }
        
        // Populate voice dropdown based on selected engine
        async function populateVoiceDropdown(engine) {
            const voiceSelect = document.getElementById('voice-select');
            voiceSelect.innerHTML = ''; // Clear existing options
            
            const voices = availableVoices[engine] || [];
            
            if (voices.length === 0) {
                voiceSelect.innerHTML = '<option value="">No voices available</option>';
                return;
            }
            
            // Group voices by gender for better organization
            const groupedVoices = {
                female: voices.filter(v => v.gender === 'female'),
                male: voices.filter(v => v.gender === 'male'),
                other: voices.filter(v => !['female', 'male'].includes(v.gender))
            };
            
            // Add female voices
            if (groupedVoices.female.length > 0) {
                const femaleGroup = document.createElement('optgroup');
                femaleGroup.label = 'Female Voices';
                groupedVoices.female.forEach(voice => {
                    const option = document.createElement('option');
                    option.value = voice.id;
                    option.textContent = voice.name;
                    option.title = voice.description || '';
                    femaleGroup.appendChild(option);
                });
                voiceSelect.appendChild(femaleGroup);
            }
            
            // Add male voices
            if (groupedVoices.male.length > 0) {
                const maleGroup = document.createElement('optgroup');
                maleGroup.label = 'Male Voices';
                groupedVoices.male.forEach(voice => {
                    const option = document.createElement('option');
                    option.value = voice.id;
                    option.textContent = voice.name;
                    option.title = voice.description || '';
                    maleGroup.appendChild(option);
                });
                voiceSelect.appendChild(maleGroup);
            }
            
            // Add other voices
            if (groupedVoices.other.length > 0) {
                const otherGroup = document.createElement('optgroup');
                otherGroup.label = 'Other Voices';
                groupedVoices.other.forEach(voice => {
                    const option = document.createElement('option');
                    option.value = voice.id;
                    option.textContent = voice.name;
                    option.title = voice.description || '';
                    otherGroup.appendChild(option);
                });
                voiceSelect.appendChild(otherGroup);
            }
            
            // Set default selection
            if (engine === 'chatterbox') {
                const defaultVoice = voices.find(v => v.id === 'female_expressive') || voices[0];
                if (defaultVoice) {
                    voiceSelect.value = defaultVoice.id;
                    voiceConfig.chatterbox.selectedVoice = defaultVoice.id;
                }
            } else if (engine === 'edge_tts') {
                const defaultVoice = voices.find(v => v.id === 'en-US-AriaNeural') || voices[0];
                if (defaultVoice) {
                    voiceSelect.value = defaultVoice.id;
                    voiceConfig.edge.selectedVoice = defaultVoice.id;
                }
            } else if (engine === 'elevenlabs') {
                const defaultVoice = voices.find(v => v.id === 'pNInz6obpgDQGcFmaJgB') || voices[0];
                if (defaultVoice) {
                    voiceSelect.value = defaultVoice.id;
                    voiceConfig.elevenlabs.selectedVoice = defaultVoice.id;
                }
            }
            
            console.log(`üéµ Populated ${voices.length} voices for ${engine}`);
        }
        
        // Handle voice selection change
        function updateVoiceSelection() {
            const selectedVoice = document.getElementById('voice-select').value;
            const engine = voiceConfig.selectedEngine;
            
            if (engine === 'chatterbox') {
                voiceConfig.chatterbox.selectedVoice = selectedVoice;
            } else if (engine === 'edge_tts') {
                voiceConfig.edge.selectedVoice = selectedVoice;
            } else if (engine === 'elevenlabs') {
                voiceConfig.elevenlabs.selectedVoice = selectedVoice;
            }
            
            updateVoicePreview();
            console.log(`üéµ Selected voice: ${selectedVoice} for engine: ${engine}`);
        }

        // Check voice service status and populate engines

        async function checkVoiceStatus() {
            try {
                const response = await fetch('/api/alfred/health');
                const data = await response.json();
              
                console.log('üîç Health endpoint response:', data);
                
                // Extract TTS engine information from the health response
                const ttsDetails = data.services?.tts?.details;
                const availableEnginesList = data.available_engines || [];
                
                if (ttsDetails && (ttsDetails.engines || availableEnginesList.length > 0)) {
                    await populateEnginesFromHealth(ttsDetails.engines, ttsDetails.engines_dict, availableEnginesList);
                }
                
                // Update status based on available engines
                if (availableEnginesList.length > 0) {
                    updateStatus(`‚úÖ ${availableEnginesList.length} Voice Engine${availableEnginesList.length > 1 ? 's' : ''} Available: ${availableEnginesList.join(', ')}`);
                    document.querySelector('.status-indicator').style.background = '#4CAF50';
                } else {
                    updateStatus('‚ùå No Voice Engines Available');
                    document.querySelector('.status-indicator').style.background = '#f44336';
                }
            } catch (error) {
                console.error('Failed to check voice status:', error);
                updateStatus('‚ùå Voice Service Connection Error');
                document.querySelector('.status-indicator').style.background = '#f44336';
            }
        }
        
        // Populate engines dropdown from health endpoint
        async function populateEnginesFromHealth(engines, enginesDict, availableEnginesList) {
            const engineSelect = document.getElementById('voice-engine');
            
            // Clear existing options except the loading one
            engineSelect.innerHTML = '';
            
            // Use the available_engines list from the health response if provided
            let engineNames = availableEnginesList || [];
            
            // If no available_engines list, fall back to parsing engines array
            if (engineNames.length === 0 && engines) {
                engineNames = engines.filter(engine => engine.available).map(engine => engine.name);
            }
            
            if (engineNames.length === 0) {
                engineSelect.innerHTML = '<option value="">No engines available</option>';
                return;
            }
            
            // Map engine names to user-friendly labels
            const engineLabels = {
                'kokoro': 'Kokoro TTS (Neural Synthesis)',
                'chatterbox': 'Chatterbox TTS (High Quality + Voice Cloning)',
                'silero': 'Silero Neural TTS (GPU Accelerated)', 
                'edge_tts': 'Edge TTS (Fast + Many Voices)',
                'gtts': 'Google TTS (Cloud)',
                'espeak': 'eSpeak TTS (Offline)',
                'elevenlabs': 'ElevenLabs (Premium + Ultra Fast)'
            };
            
            // Separate engines into Local and Cloud categories
            const localEngines = ['kokoro', 'chatterbox', 'silero', 'espeak'];
            const cloudEngines = ['edge_tts', 'gtts', 'elevenlabs'];
            
            const localAvailable = engineNames.filter(name => localEngines.includes(name));
            const cloudAvailable = engineNames.filter(name => cloudEngines.includes(name));
            
            // Add local engines section
            if (localAvailable.length > 0) {
                const localGroup = document.createElement('optgroup');
                localGroup.label = 'Local Engines';
                localAvailable.forEach(engineName => {
                    const option = document.createElement('option');
                    option.value = engineName;
                    option.textContent = engineLabels[engineName] || `${engineName} TTS`;
                    localGroup.appendChild(option);
                });
                engineSelect.appendChild(localGroup);
            }
            
            // Add cloud engines section (including ElevenLabs)
            const allCloudEngines = [...cloudAvailable];
            if (!allCloudEngines.includes('elevenlabs')) {
                allCloudEngines.push('elevenlabs'); // Always show ElevenLabs as option
            }
            
            if (allCloudEngines.length > 0) {
                const cloudGroup = document.createElement('optgroup');
                cloudGroup.label = 'Cloud Engines';
                allCloudEngines.forEach(engineName => {
                    const option = document.createElement('option');
                    option.value = engineName;
                    option.textContent = engineLabels[engineName] || `${engineName} TTS`;
                    cloudGroup.appendChild(option);
                });
                engineSelect.appendChild(cloudGroup);
            }
            
            // Set the first available engine as selected
            if (engineNames.length > 0) {
                engineSelect.value = engineNames[0];
                voiceConfig.selectedEngine = engineNames[0];
            }
            
            console.log(`üéõÔ∏è Populated ${engineNames.length} engines:`, engineNames);
            
            // Load voices for the selected engine
            const selectedEngine = engineSelect.value;
            if (selectedEngine) {
                await switchEngine(); // This will populate voices and update UI
            }
        }

        // Edge TTS slider update functions
        function updateSpeed(value) {
            voiceConfig.edge.speed = value / 100;
            document.getElementById('speed-value').textContent = value + '%';
            updateVoicePreview();
        }
        
        function updatePitch(value) {
            voiceConfig.edge.pitch = parseInt(value);
            document.getElementById('pitch-value').textContent = value + 'Hz';
            updateVoicePreview();
        }
        
        function updateVolume(value) {
            voiceConfig.edge.volume = parseInt(value);
            document.getElementById('volume-value').textContent = value + '%';
            updateVoicePreview();
        }

        // Update emotion level (Chatterbox)
        function updateEmotion(value) {
            voiceConfig.chatterbox.exaggeration = value / 100;
            document.getElementById('emotion-value').textContent = value + '%';
            updateVoicePreview();
        }

        // Update pacing (Chatterbox)
        function updatePacing(value) {
            voiceConfig.chatterbox.cfg_weight = value / 100;
            document.getElementById('pacing-value').textContent = value + '%';
            updateVoicePreview();
        }

        // ElevenLabs slider update functions
        function updateStability(value) {
            voiceConfig.elevenlabs.stability = value / 100;
            document.getElementById('stability-value').textContent = value + '%';
            updateVoicePreview();
        }
        
        function updateSimilarity(value) {
            voiceConfig.elevenlabs.similarity_boost = value / 100;
            document.getElementById('similarity-value').textContent = value + '%';
            updateVoicePreview();
        }
        
        function updateStyle(value) {
            voiceConfig.elevenlabs.style = value / 100;
            document.getElementById('style-value').textContent = value + '%';
            updateVoicePreview();
        }
        
        function updateModel(value) {
            voiceConfig.elevenlabs.model_id = value;
            updateVoicePreview();
        }

        // Toggle TTS on/off
        function toggleTTS() {
            voiceConfig.enabled = document.getElementById('tts-enabled').checked;
            const engineName = voiceConfig.selectedEngine === 'chatterbox' ? 'Chatterbox TTS' : 
                             voiceConfig.selectedEngine === 'edge_tts' ? 'Edge TTS' : 'ElevenLabs';
            updateStatus(voiceConfig.enabled ? `${engineName} Enabled` : 'TTS Disabled');
            updateVoicePreview();
        }

        // Handle voice sample file upload
        function handleVoiceSampleUpload(input) {
            const file = input.files[0];
            if (!file) {
                voiceConfig.chatterbox.voiceCloning.audioFile = null;
                voiceConfig.chatterbox.voiceCloning.fileName = null;
                updateVoiceCloneStatus('No voice sample uploaded. Using default Chatterbox voices.');
                return;
            }

            // Validate file type
            const validTypes = ['audio/wav', 'audio/mpeg', 'audio/mp3', 'audio/ogg', 'audio/webm'];
            if (!validTypes.includes(file.type)) {
                alert('Please upload a valid audio file (WAV, MP3, OGG, or WebM)');
                input.value = '';
                return;
            }

            // Validate file size (max 10MB)
            if (file.size > 10 * 1024 * 1024) {
                alert('File size must be less than 10MB');
                input.value = '';
                return;
            }

            voiceConfig.chatterbox.voiceCloning.audioFile = file;
            voiceConfig.chatterbox.voiceCloning.fileName = file.name;
            
            updateVoiceCloneStatus(`Voice sample loaded: ${file.name} (${(file.size / 1024 / 1024).toFixed(2)}MB). Enable voice cloning to use it.`);
            
            console.log('üé§ Voice sample uploaded:', file.name, file.type);
        }

        // Toggle voice cloning mode
        function toggleVoiceCloning() {
            const enabled = document.getElementById('voice-cloning-enabled').checked;
            
            if (enabled && !voiceConfig.chatterbox.voiceCloning.audioFile) {
                alert('Please upload a voice sample first!');
                document.getElementById('voice-cloning-enabled').checked = false;
                return;
            }
            
            voiceConfig.chatterbox.voiceCloning.enabled = enabled;
            
            // Show/hide voice cloning test button
            const testCloneBtn = document.getElementById('test-clone-btn');
            if (enabled) {
                testCloneBtn.style.display = 'inline-block';
                updateVoiceCloneStatus(`‚úÖ Voice cloning active with ${voiceConfig.chatterbox.voiceCloning.fileName}. Your AI will speak in the uploaded voice!`);
                updateStatus('üé§ Voice cloning mode enabled - AI will use your uploaded voice sample');
            } else {
                testCloneBtn.style.display = 'none';
                updateVoiceCloneStatus(`Voice sample available: ${voiceConfig.chatterbox.voiceCloning.fileName}. Enable voice cloning to use it.`);
                updateStatus('üé≠ Using default Chatterbox voices');
            }
            
            updateVoicePreview();
        }

        // Test voice cloning specifically
        async function testVoiceCloning() {
            if (!voiceConfig.chatterbox.voiceCloning.enabled || !voiceConfig.chatterbox.voiceCloning.audioFile) {
                updateStatus('‚ùå Voice cloning not enabled or no audio file uploaded');
                return;
            }

            const testText = `Hello! This is a test of voice cloning using your uploaded voice sample. I should now sound like the person in your audio file, with ${Math.round(voiceConfig.chatterbox.exaggeration * 100)} percent emotional expression.`;
            
            updateStatus('üé§ Testing voice cloning...');
            
            try {
                await synthesizeVoice(testText);
                updateStatus('‚úÖ Voice cloning test completed successfully!');
            } catch (error) {
                console.error('Voice cloning test failed:', error);
                updateStatus('‚ùå Voice cloning test failed - check console for details');
            }
        }

        // Update voice clone status display
        function updateVoiceCloneStatus(message) {
            document.getElementById('voice-clone-status').textContent = message;
        }

        // Toggle microphone recording for voice sample
        async function toggleRecording() {
            if (voiceConfig.chatterbox.recording.isRecording) {
                stopRecording();
            } else {
                await startRecording();
            }
        }

        // Start recording voice sample
        async function startRecording() {
            try {
                // Request microphone access
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        sampleRate: 44100,
                        channelCount: 1,
                        volume: 1.0,
                        echoCancellation: true,
                        noiseSuppression: true
                    } 
                });

                // Set up MediaRecorder
                voiceConfig.chatterbox.recording.mediaRecorder = new MediaRecorder(stream);
                voiceConfig.chatterbox.recording.audioChunks = [];
                voiceConfig.chatterbox.recording.startTime = Date.now();

                // Update UI for recording state
                voiceConfig.chatterbox.recording.isRecording = true;
                document.getElementById('record-btn-text').textContent = '‚èπÔ∏è Stop Recording';
                document.getElementById('record-sample-btn').classList.add('recording-active');
                document.getElementById('recording-controls').style.display = 'block';

                // Start timer and progress
                updateRecordingTimer();

                // Handle recording data
                voiceConfig.chatterbox.recording.mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        voiceConfig.chatterbox.recording.audioChunks.push(event.data);
                    }
                };

                // Handle recording stop
                voiceConfig.chatterbox.recording.mediaRecorder.onstop = () => {
                    const audioBlob = new Blob(voiceConfig.chatterbox.recording.audioChunks, { 
                        type: 'audio/wav' 
                    });
                    
                    // Create a File object from the blob
                    const audioFile = new File([audioBlob], `voice_sample_${Date.now()}.wav`, {
                        type: 'audio/wav'
                    });
                    
                    // Use the recorded audio as voice sample
                    voiceConfig.chatterbox.voiceCloning.audioFile = audioFile;
                    voiceConfig.chatterbox.voiceCloning.fileName = audioFile.name;
                    
                    const duration = ((Date.now() - voiceConfig.chatterbox.recording.startTime) / 1000).toFixed(1);
                    updateVoiceCloneStatus(`Voice sample recorded: ${duration}s audio. Enable voice cloning to use it.`);
                    
                    // Stop all tracks to release microphone
                    stream.getTracks().forEach(track => track.stop());
                    
                    console.log('üé§ Voice sample recorded:', audioFile.name, `${duration}s`);
                };

                // Start recording
                voiceConfig.chatterbox.recording.mediaRecorder.start();
                updateStatus('üé§ Recording voice sample... Speak clearly for 3-10 seconds');

                // Auto-stop after max duration
                setTimeout(() => {
                    if (voiceConfig.chatterbox.recording.isRecording) {
                        stopRecording();
                    }
                }, voiceConfig.chatterbox.recording.maxDuration);

            } catch (error) {
                console.error('Failed to start recording:', error);
                updateStatus('‚ùå Microphone access denied or not available');
                alert('Could not access microphone. Please check permissions and try again.');
            }
        }

        // Stop recording voice sample
        function stopRecording() {
            if (!voiceConfig.chatterbox.recording.isRecording) return;

            voiceConfig.chatterbox.recording.isRecording = false;
            
            // Stop MediaRecorder
            if (voiceConfig.chatterbox.recording.mediaRecorder && 
                voiceConfig.chatterbox.recording.mediaRecorder.state !== 'inactive') {
                voiceConfig.chatterbox.recording.mediaRecorder.stop();
            }

            // Clear timer
            if (voiceConfig.chatterbox.recording.timer) {
                clearInterval(voiceConfig.chatterbox.recording.timer);
            }

            // Reset UI
            document.getElementById('record-btn-text').textContent = 'üéôÔ∏è Record Sample';
            document.getElementById('record-sample-btn').classList.remove('recording-active');
            document.getElementById('recording-controls').style.display = 'none';
            document.getElementById('recording-timer').textContent = '00:00';
            document.getElementById('recording-progress').style.width = '0%';

            updateStatus('üéµ Recording completed - voice sample ready for cloning');
        }

        // Update recording timer and progress
        function updateRecordingTimer() {
            voiceConfig.chatterbox.recording.timer = setInterval(() => {
                if (!voiceConfig.chatterbox.recording.isRecording) return;

                const elapsed = Date.now() - voiceConfig.chatterbox.recording.startTime;
                const seconds = Math.floor(elapsed / 1000);
                const progress = Math.min((elapsed / voiceConfig.chatterbox.recording.maxDuration) * 100, 100);

                // Update timer display
                const minutes = Math.floor(seconds / 60);
                const secs = seconds % 60;
                document.getElementById('recording-timer').textContent = 
                    `${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;

                // Update progress bar
                document.getElementById('recording-progress').style.width = `${progress}%`;

                // Auto-stop if max duration reached
                if (elapsed >= voiceConfig.chatterbox.recording.maxDuration) {
                    stopRecording();
                }
            }, 100);
        }

        // Update voice preview description
        function updateVoicePreview() {
            if (!voiceConfig.enabled) {
                document.getElementById('voice-preview').textContent = '‚ö†Ô∏è Voice synthesis is currently disabled.';
                return;
            }
            
            const engine = voiceConfig.selectedEngine;
            let preview = '';
            
            if (engine === 'chatterbox') {
                const emotionLevel = Math.round(voiceConfig.chatterbox.exaggeration * 100);
                const pacing = Math.round(voiceConfig.chatterbox.cfg_weight * 100);
                const selectedVoice = voiceConfig.chatterbox.selectedVoice;
                
                // Voice cloning mode
                if (voiceConfig.chatterbox.voiceCloning.enabled && voiceConfig.chatterbox.voiceCloning.fileName) {
                    preview = `üé§ Voice Cloning Active: Using uploaded voice sample "${voiceConfig.chatterbox.voiceCloning.fileName}" with ${emotionLevel}% emotion and ${pacing}% pacing. The AI will speak in your custom voice!`;
                } else {
                    // Get voice info from available voices
                    const voiceInfo = availableVoices.chatterbox.find(v => v.id === selectedVoice);
                    const voiceName = voiceInfo ? voiceInfo.name : selectedVoice;
                    
                    preview = `Current: ${voiceName} with ${emotionLevel}% emotion and ${pacing}% pacing. High-quality neural synthesis with advanced emotion control. ‚ú® Optimized for engaging, natural speech.`;
                }
            } else if (engine === 'edge_tts') {
                const speed = Math.round(voiceConfig.edge.speed * 100);
                const pitch = voiceConfig.edge.pitch;
                const volume = voiceConfig.edge.volume;
                const selectedVoice = voiceConfig.edge.selectedVoice;
                
                // Get voice info from available voices
                const voiceInfo = availableVoices.edge_tts.find(v => v.id === selectedVoice);
                const voiceName = voiceInfo ? voiceInfo.name : selectedVoice;
                
                preview = `Current: ${voiceName} at ${speed}% speed`;
                if (pitch !== 0) preview += `, ${pitch > 0 ? '+' : ''}${pitch}Hz pitch`;
                if (volume !== 0) preview += `, ${volume > 0 ? '+' : ''}${volume}% volume`;
                preview += '. Fast response neural voice perfect for real-time conversation. ‚ö° Instant synthesis.';
            } else if (engine === 'elevenlabs') {
                const stability = Math.round(voiceConfig.elevenlabs.stability * 100);
                const similarity = Math.round(voiceConfig.elevenlabs.similarity_boost * 100);
                const style = Math.round(voiceConfig.elevenlabs.style * 100);
                const selectedVoice = voiceConfig.elevenlabs.selectedVoice;
                const model = voiceConfig.elevenlabs.model_id;
                
                // Get voice info from available voices
                const voiceInfo = availableVoices.elevenlabs.find(v => v.id === selectedVoice);
                const voiceName = voiceInfo ? voiceInfo.name : selectedVoice;
                
                preview = `Current: ${voiceName} using ${model}`;
                preview += ` with ${stability}% stability, ${similarity}% similarity`;
                if (style > 0) preview += `, ${style}% style`;
                preview += '. Premium quality ultra-fast synthesis. üöÄ Professional grade voice.';
            } else if (engine === 'kokoro') {
                const emotionLevel = Math.round(voiceConfig.chatterbox.exaggeration * 100);
                const selectedVoice = voiceConfig.chatterbox.selectedVoice;
                preview = `Current: Kokoro TTS with ${selectedVoice} voice and ${emotionLevel}% emotion. High-quality neural synthesis engine. üéµ Natural voice generation.`;
            } else if (engine === 'silero') {
                const emotionLevel = Math.round(voiceConfig.chatterbox.exaggeration * 100);
                const selectedVoice = voiceConfig.chatterbox.selectedVoice;
                preview = `Current: Silero Neural TTS with ${selectedVoice} voice and ${emotionLevel}% emotion. GPU-accelerated neural synthesis engine. üéµ Natural voice generation.`;
            } else if (engine === 'gtts') {
                preview = `Current: Google Text-to-Speech with cloud-based synthesis. Reliable online voice generation with natural pronunciation. üåê Cloud-powered TTS.`;
            } else if (engine === 'espeak') {
                preview = `Current: eSpeak TTS with offline synthesis. Lightweight and reliable text-to-speech engine. üíª Always available offline.`;
            } else {
                preview = `Current: ${engine} TTS engine selected. Voice synthesis ready for testing.`;

            }
            
            document.getElementById('voice-preview').textContent = preview;
        }

        // Universal voice synthesis function (handles all engines)

        async function synthesizeVoice(text) {
            console.log('üéµ synthesizeVoice called with text:', text);
            
            if (!voiceConfig.enabled) {
                console.log('Voice synthesis disabled, skipping');
                return;
            }

            const engine = voiceConfig.selectedEngine;
            
            try {
                if (engine === 'chatterbox') {
                    await synthesizeWithChatterbox(text);
                } else if (engine === 'edge_tts') {
                    await synthesizeWithEdge(text);
                } else if (engine === 'elevenlabs') {
                    await synthesizeWithElevenLabs(text);
                } else if (engine === 'maya1' || engine === 'kokoro' || engine === 'silero' || engine === 'gtts' || engine === 'espeak') {
                    await synthesizeWithGenericEngine(text, engine);

                } else {
                    throw new Error(`Unknown engine: ${engine}`);
                }
            } catch (error) {
                console.error(`Voice synthesis failed with ${engine}:`, error);
                updateStatus(`‚ùå ${engine} synthesis failed - trying fallback...`);
                
                // Fallback to browser TTS if synthesis fails

                if ('speechSynthesis' in window) {
                    const utterance = new SpeechSynthesisUtterance(text);
                    utterance.rate = 0.9;
                    utterance.pitch = 1.0;
                    utterance.volume = 0.8;
                    speechSynthesis.speak(utterance);
                    updateStatus('üîä Using browser TTS fallback');
                }
            }
        }
        
        // Generic synthesis for simple engines (kokoro, gtts, espeak)
        async function synthesizeWithGenericEngine(text, engine) {
            try {
                updateStatus(`üéµ Synthesizing with ${engine}...`);
                
                // Prepare form data with all required parameters
                const formData = new FormData();
                formData.append('text', text);
                formData.append('engine', engine);
                
                // Required parameters for all engines
                formData.append('gender', 'female');
                formData.append('emotion', 'neutral');
                formData.append('exaggeration', voiceConfig.chatterbox.exaggeration.toString());
                formData.append('cfg_weight', voiceConfig.chatterbox.cfg_weight.toString());
                formData.append('voice_cloning', false);
                
                // Add engine-specific voice configuration
                if (engine === 'kokoro') {
                    // Kokoro might support voice selection
                    formData.append('voice_id', voiceConfig.chatterbox.selectedVoice || 'default');
                    formData.append('speed', '1.0');
                } else if (engine === 'gtts') {
                    // Google TTS supports language/voice
                    formData.append('voice_id', 'en');
                    formData.append('speed', '1.0');
                } else if (engine === 'espeak') {
                    // eSpeak basic parameters
                    formData.append('voice_id', 'en');
                    formData.append('speed', '1.0'); // Let backend handle speed conversion
                }
                
                const response = await fetch('/api/alfred/synthesize', {
                    method: 'POST',
                    body: formData
                });

                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }

                const audioBlob = await response.blob();
                const audioUrl = URL.createObjectURL(audioBlob);
                
                // Stop any currently playing audio and play new audio
                if (currentAudio) {
                    currentAudio.pause();
                    currentAudio.currentTime = 0;
                }
                
                currentAudio = new Audio(audioUrl);
                currentAudio.play();
                
                currentAudio.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    updateStatus(`üéµ ${engine} Ready`);
                };
                
                updateStatus(`üîä Playing ${engine} synthesis...`);
                
            } catch (error) {
                console.error(`${engine} synthesis failed:`, error);
                throw error;
            }
        }
        
        
        // Synthesize with Edge TTS (new function)
        async function synthesizeWithEdge(text) {
            try {
                updateStatus('‚ö° Synthesizing with Edge TTS...');
                
                // Prepare form data with all required parameters
                const formData = new FormData();
                formData.append('text', text);
                formData.append('engine', 'edge_tts');
                formData.append('voice_id', voiceConfig.edge.selectedVoice);
                formData.append('speed', voiceConfig.edge.speed.toString());
                
                // Required parameters with defaults
                formData.append('gender', 'female');
                formData.append('emotion', 'neutral');
                formData.append('exaggeration', (voiceConfig.edge.pitch / 10 + 0.5).toString()); // Convert pitch to exaggeration range
                formData.append('cfg_weight', (voiceConfig.edge.volume / 20 + 0.5).toString());  // Convert volume to cfg_weight range
                formData.append('voice_cloning', false);

                
                const response = await fetch('/api/alfred/synthesize', {
                    method: 'POST',
                    body: formData
                });

                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }

                const audioBlob = await response.blob();
                const audioUrl = URL.createObjectURL(audioBlob);
                
                // Stop any currently playing audio and play new audio
                if (currentAudio) {
                    currentAudio.pause();
                    currentAudio.currentTime = 0;
                }
                
                currentAudio = new Audio(audioUrl);
                currentAudio.play();
                
                currentAudio.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    updateStatus('‚ö° Edge TTS Ready');
                };
                
                updateStatus('üîä Playing Edge TTS synthesis...');
                
            } catch (error) {
                console.error('Edge TTS synthesis failed:', error);
                throw error;
            }
        }

        // Synthesize with ElevenLabs
        async function synthesizeWithElevenLabs(text) {
            try {
                updateStatus('üöÄ Synthesizing with ElevenLabs...');
                
                // Prepare JSON data for ElevenLabs API
                const requestData = {
                    text: text,
                    voice_settings: {
                        stability: voiceConfig.elevenlabs.stability,
                        similarity_boost: voiceConfig.elevenlabs.similarity_boost,
                        style: voiceConfig.elevenlabs.style,
                        use_speaker_boost: voiceConfig.elevenlabs.use_speaker_boost
                    },
                    model_id: voiceConfig.elevenlabs.model_id
                };
                
                const response = await fetch(`/api/alfred/elevenlabs/${voiceConfig.elevenlabs.selectedVoice}/synthesize`, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify(requestData)
                });

                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }

                const audioBlob = await response.blob();
                const audioUrl = URL.createObjectURL(audioBlob);
                
                // Stop any currently playing audio and play new audio
                if (currentAudio) {
                    currentAudio.pause();
                    currentAudio.currentTime = 0;
                }
                
                currentAudio = new Audio(audioUrl);
                currentAudio.play();
                
                currentAudio.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    updateStatus('üöÄ ElevenLabs Ready');
                };
                
                updateStatus('üîä Playing ElevenLabs synthesis...');
                
            } catch (error) {
                console.error('ElevenLabs synthesis failed:', error);
                throw error;
            }
        }

        // Test current voice settings
        async function testChatterboxVoice() {
            if (!voiceConfig.enabled) {
                updateStatus('‚ùå Voice synthesis is disabled. Enable it first.');
                return;
            }

            const engine = voiceConfig.selectedEngine;
            let testText = '';
            
            if (engine === 'chatterbox') {
                const emotionLevel = Math.round(voiceConfig.chatterbox.exaggeration * 100);
                testText = `Hello! I'm your AI assistant powered by Chatterbox TTS. This voice has ${emotionLevel} percent emotion and advanced neural synthesis with voice cloning capabilities.`;
            } else if (engine === 'edge_tts') {
                const speed = Math.round(voiceConfig.edge.speed * 100);
                testText = `Hello! I'm your AI assistant using Edge TTS at ${speed} percent speed. This is perfect for fast, real-time conversation with natural neural voices.`;
            } else if (engine === 'elevenlabs') {
                const stability = Math.round(voiceConfig.elevenlabs.stability * 100);
                const similarity = Math.round(voiceConfig.elevenlabs.similarity_boost * 100);
                testText = `Hello! I'm your AI assistant powered by ElevenLabs premium TTS. This voice uses ${stability} percent stability and ${similarity} percent similarity for ultra-fast, professional quality synthesis.`;
            } else if (engine === 'kokoro') {
                testText = `Hello! I'm your AI assistant using Kokoro TTS. This is a high-quality neural text-to-speech engine with natural voice synthesis.`;
            } else if (engine === 'silero') {
                testText = `Hello! I'm your AI assistant using Silero Neural TTS. This is a high-quality GPU-accelerated text-to-speech engine with natural voice synthesis.`;
            } else if (engine === 'gtts') {
                testText = `Hello! I'm your AI assistant using Google Text-to-Speech. This is a cloud-based service with excellent voice quality.`;
            } else if (engine === 'espeak') {
                testText = `Hello! I'm your AI assistant using eSpeak TTS. This is a reliable offline text-to-speech engine.`;
            } else {
                testText = `Hello! I'm your AI assistant using ${engine} TTS. Testing voice synthesis with the selected engine.`;
            }
            
            const engineNames = {
                'kokoro': 'Kokoro',
                'chatterbox': 'Chatterbox',
                'edge_tts': 'Edge TTS',
                'elevenlabs': 'ElevenLabs',
                'silero': 'Silero Neural TTS',
                'gtts': 'Google TTS',
                'espeak': 'eSpeak'
            };
            
            updateStatus(`üéµ Testing ${engineNames[engine] || engine} voice...`);

            
            try {
                await synthesizeVoice(testText);
                updateStatus('‚úÖ Voice test completed successfully!');
            } catch (error) {
                console.error('Voice test failed:', error);
                updateStatus('‚ùå Voice test failed - check console for details');
            }
        }

        // Synthesize speech using Chatterbox TTS
        async function synthesizeWithChatterbox(text) {
            if (!voiceConfig.enabled) {
                console.log('TTS disabled, skipping synthesis');
                return;
            }

            try {
                updateStatus(voiceConfig.chatterbox.voiceCloning.enabled ? 
                    'üé§ Synthesizing with voice cloning...' : 
                    'üéµ Synthesizing with Chatterbox TTS...');

                // Always use FormData with all required parameters
                const formData = new FormData();
                formData.append('text', text);
                formData.append('engine', 'chatterbox');
                formData.append('voice_id', voiceConfig.chatterbox.selectedVoice);
                formData.append('speed', '1.0');
                
                // Chatterbox-specific parameters
                formData.append('gender', 'female');
                formData.append('emotion', 'neutral');
                formData.append('exaggeration', voiceConfig.chatterbox.exaggeration.toString());
                formData.append('cfg_weight', voiceConfig.chatterbox.cfg_weight.toString());
                
                if (voiceConfig.chatterbox.voiceCloning.enabled && voiceConfig.chatterbox.voiceCloning.audioFile) {
                    // Voice cloning mode - add audio file
                    formData.append('voice_cloning', true);

                    formData.append('audio_prompt', voiceConfig.chatterbox.voiceCloning.audioFile);
                    console.log('üé§ Adding voice cloning audio file to request:', voiceConfig.chatterbox.voiceCloning.audioFile.name);
                } else {
                    // Regular synthesis mode
                    formData.append('voice_cloning', false);
                }
                

                // Create abort controller for timeout (2 minutes for Chatterbox synthesis)
                const controller = new AbortController();
                const timeoutId = setTimeout(() => controller.abort(), 120000);
                
                const response = await fetch('/api/alfred/synthesize', {
                    method: 'POST',
                    body: formData,

                    signal: controller.signal
                });
                
                clearTimeout(timeoutId);

                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }

                const audioBlob = await response.blob();
                const audioUrl = URL.createObjectURL(audioBlob);
                
                // Stop any currently playing audio
                if (currentAudio) {
                    currentAudio.pause();
                    currentAudio.currentTime = 0;
                }
                
                // Play the new audio
                currentAudio = new Audio(audioUrl);
                currentAudio.play();
                
                currentAudio.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    updateStatus('üé≠ Chatterbox TTS Ready');
                };
                
                updateStatus('üîä Playing Chatterbox synthesis...');
                
            } catch (error) {
                console.error('Chatterbox synthesis failed:', error);
                
                // Handle timeout specifically
                if (error.name === 'AbortError') {
                    updateStatus('‚è±Ô∏è Chatterbox synthesis taking longer than expected - trying fallback...');
                } else {
                    updateStatus('‚ùå Synthesis failed - trying fallback...');
                }
                
                // Fallback to browser TTS if Chatterbox fails
                if ('speechSynthesis' in window) {
                    const utterance = new SpeechSynthesisUtterance(text);
                    utterance.rate = 0.9;
                    utterance.pitch = 1.0;
                    utterance.volume = 0.8;
                    speechSynthesis.speak(utterance);
                    updateStatus('üîä Using browser TTS fallback');
                }
            }
        }

        // Real microphone functionality with STT integration
        async function toggleMicrophone() {
            if (isListening) {
                stopListening();
            } else {
                await startListening();
            }
        }

        async function startListening() {
            try {
                // Request microphone access
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        sampleRate: 16000,  // Optimal for Whisper STT  
                        channelCount: 1,
                        volume: 1.0,
                        echoCancellation: true,
                        noiseSuppression: true
                    } 
                });

                // Set up MediaRecorder
                mediaRecorder = new MediaRecorder(stream, {
                    mimeType: 'audio/webm;codecs=opus'
                });
                audioChunks = [];

                // Update UI
                isListening = true;
                const btn = document.getElementById('microphone-btn');
                const status = document.getElementById('mic-status');
                btn.classList.add('listening');
                status.textContent = 'Listening...';
                updateStatus('üé§ Listening for voice input...');

                // Handle recording data
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                // Handle recording stop
                mediaRecorder.onstop = async () => {
                    updateStatus('üîÑ Processing speech...');
                    
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    
                    // Send to chat endpoint for STT + AI response
                    await sendAudioToChat(audioBlob);
                    
                    // Stop all tracks to release microphone
                    stream.getTracks().forEach(track => track.stop());
                };

                // Start recording
                mediaRecorder.start();

                // Auto-stop after 30 seconds max
                setTimeout(() => {
                    if (isListening) {
                        stopListening();
                    }
                }, 30000);

            } catch (error) {
                console.error('Failed to start listening:', error);
                updateStatus('‚ùå Microphone access denied or not available');
                alert('Could not access microphone. Please check permissions and try again.');
            }
        }

        function stopListening() {
            if (!isListening) return;

            isListening = false;
            
            // Stop MediaRecorder
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }

            // Reset UI
            const btn = document.getElementById('microphone-btn');
            const status = document.getElementById('mic-status');
            btn.classList.remove('listening');
            status.textContent = 'Click to Start';
            updateStatus('üîÑ Processing...');
        }

        // Send recorded audio to chat endpoint for STT + AI response
        async function sendAudioToChat(audioBlob) {
            console.log('üöÄ sendAudioToChat called with blob size:', audioBlob.size);
            try {
                // Stop any currently playing audio to prevent conflicts
                stopAll();
                
                updateStatus('üéØ Processing audio...');
                
                // Create FormData with audio file
                const formData = new FormData();
                formData.append('audio', audioBlob, 'recording.webm');
                
                // Add conversation history if available
                if (conversationHistory.length > 0) {
                    formData.append('conversationHistory', JSON.stringify(conversationHistory));
                    console.log('üìù Added conversation history:', conversationHistory.length, 'messages');
                }
                
                // Add voice configuration based on user's selected engine
                const voiceSettings = {
                    engine: voiceConfig.selectedEngine,
                    enabled: voiceConfig.enabled,
                    chatterbox: {
                        selectedVoice: voiceConfig.chatterbox.selectedVoice,
                        exaggeration: voiceConfig.chatterbox.exaggeration,
                        cfg_weight: voiceConfig.chatterbox.cfg_weight,
                        voiceCloning: voiceConfig.chatterbox.voiceCloning
                    },
                    edge: {
                        selectedVoice: voiceConfig.edge.selectedVoice,
                        speed: voiceConfig.edge.speed,
                        pitch: voiceConfig.edge.pitch,
                        volume: voiceConfig.edge.volume
                    },
                    elevenlabs: {
                        selectedVoice: voiceConfig.elevenlabs.selectedVoice,
                        model_id: voiceConfig.elevenlabs.model_id,
                        stability: voiceConfig.elevenlabs.stability,
                        similarity_boost: voiceConfig.elevenlabs.similarity_boost,
                        style: voiceConfig.elevenlabs.style,
                        use_speaker_boost: voiceConfig.elevenlabs.use_speaker_boost
                    }
                };
                
                formData.append('voiceConfig', JSON.stringify(voiceSettings));
                console.log('üéµ Added voice configuration:', voiceConfig.selectedEngine, 'engine selected');

                console.log('üì° Sending request to /api/generate/chat...');
                
                // Send to chat endpoint
                const response = await fetch('/api/generate/chat', {
                    method: 'POST',
                    body: formData
                });

                console.log('üì° Response status:', response.status, response.statusText);

                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }

                const result = await response.json();
                console.log('üì° Response data:', result);
                
                // Handle the response
                if (result.success && result.dialogueId) {
                    updateStatus('üîÑ Connecting to stream...');
                    console.log('üîó Connecting to stream with dialogueId:', result.dialogueId);
                    
                    // Connect to streaming endpoint to receive the actual response
                    await connectToStream(result.dialogueId);
                    
                } else {
                    console.error('‚ùå Chat request failed:', result);
                    throw new Error(result.error || 'Chat request failed');
                }

            } catch (error) {
                console.error('‚ùå Failed to process audio:', error);
                updateStatus('‚ùå Failed to process speech - try again');
                
                // Fallback to transcript message
                addToTranscript('System', 'Failed to process audio. Please try again.');
            }
        }

        // Global variables for streaming
        let currentEventSource = null;
        let currentUserText = '';
        let currentAiResponse = '';

        async function connectToStream(dialogueId) {
            return new Promise((resolve, reject) => {
                console.log(`Connecting to stream with dialogueId: ${dialogueId}`);
                
                // Close any existing connection
                if (currentEventSource) {
                    currentEventSource.close();
                    currentEventSource = null;
                }
                
                // Reset response accumulator
                currentAiResponse = '';
                
                // Reset audio queue for new conversation
                resetAudioQueue();
                
                // Create EventSource connection
                const streamUrl = `/api/generate/open-stream?id=${dialogueId}`;
                currentEventSource = new EventSource(streamUrl);
                
                let responseStarted = false;
                let conversationComplete = false;
                
                // Handle connection open
                currentEventSource.addEventListener('open', function(event) {
                    console.log('Stream connection opened');
                    updateStatus('üéß Listening for response...');
                });
                
                // Handle connected event
                currentEventSource.addEventListener('connected', function(event) {
                    console.log('Stream connected:', event.data);
                });
                
                // Handle text chunks
                currentEventSource.addEventListener('textChunk', function(event) {
                    try {
                        const data = JSON.parse(event.data);
                        console.log('Received text chunk:', data);
                        
                        if (data.role === 'user') {
                            // Store user text for conversation history
                            currentUserText = data.text;
                            addToTranscript('You', data.text);
                        } else if (data.role === 'assistant') {
                            if (!responseStarted) {
                                responseStarted = true;
                                updateStatus('ü§ñ AI responding...');
                            }
                            // Accumulate AI response
                            currentAiResponse += data.text;
                            // Update transcript in real-time
                            updateTranscriptRealtime('Assistant', currentAiResponse);
                            
                            // Update status to show TTS preparation after substantial response
                            if (currentAiResponse.length > 100) {
                                updateStatus('üéµ Generating voice response...');
                            }
                        }
                    } catch (e) {
                        console.error('Error parsing text chunk:', e);
                    }
                });
                
                // Handle old-style complete audio (for backwards compatibility)
                currentEventSource.addEventListener('audio', function(event) {
                    try {
                        const data = JSON.parse(event.data);
                        console.log('üîä Received complete audio event:', data);
                        
                        if (data.audio) {
                            console.log('üîä Processing complete audio');
                            // Play audio chunk immediately
                            playAudioChunk(data.audio);
                        } else {
                            console.warn('üîä Audio event without audio data:', data);
                        }
                    } catch (e) {
                        console.error('üîä Error parsing complete audio:', e);
                        console.error('üîä Raw audio event data:', event.data);
                    }
                });
                
                // Handle new chunked audio 
                currentEventSource.addEventListener('audioChunk', function(event) {
                    try {
                        const data = JSON.parse(event.data);
                        console.log(`üîä Received audio chunk ${data.chunkIndex} for text: "${data.text ? data.text.substring(0, 50) : 'N/A'}..."`);
                        
                        if (data.audio) {
                            console.log(`üîä Queueing audio chunk ${data.chunkIndex} (${data.isFinal ? 'FINAL' : 'PARTIAL'})`);
                            
                            // Queue the audio chunk with metadata
                            queueAudioChunk({
                                audio: data.audio,
                                chunkIndex: data.chunkIndex,
                                text: data.text,
                                isFinal: data.isFinal
                            });
                        } else {
                            console.warn('üîä Audio chunk event without audio data:', data);
                        }
                    } catch (e) {
                        console.error('üîä Error parsing audio chunk:', e);
                        console.error('üîä Raw audio chunk event data:', event.data);
                    }
                });
                
                // Handle completion
                currentEventSource.addEventListener('done', function(event) {
                    console.log('Stream completed');
                    conversationComplete = true;
                    
                    // Add to conversation history
                    if (currentUserText && currentAiResponse) {
                        conversationHistory.push(
                            { role: 'user', content: currentUserText },
                            { role: 'assistant', content: currentAiResponse }
                        );
                    }
                    
                    updateStatus('‚úÖ Conversation complete - audio ready');
                    
                    // Close connection
                    if (currentEventSource) {
                        currentEventSource.close();
                        currentEventSource = null;
                    }
                    
                    resolve();
                });
                
                // Handle errors
                currentEventSource.addEventListener('error', function(event) {
                    console.error('Stream error:', event);
                    updateStatus('‚ùå Stream connection error');
                    
                    if (currentEventSource) {
                        currentEventSource.close();
                        currentEventSource = null;
                    }
                    
                    if (!conversationComplete) {
                        reject(new Error('Stream connection failed'));
                    }
                });
                
                // Handle generic messages
                currentEventSource.onmessage = function(event) {
                    console.log('Generic message:', event.data);
                };
                
                // Timeout after 2 minutes (120 seconds) to allow for TTS synthesis
                setTimeout(() => {
                    if (!conversationComplete) {
                        console.warn('Stream timeout');
                        updateStatus('‚è±Ô∏è Response timeout');
                        
                        if (currentEventSource) {
                            currentEventSource.close();
                            currentEventSource = null;
                        }
                        
                        reject(new Error('Stream timeout'));
                    }
                }, 120000);
            });
        }

        // Helper function to update transcript in real-time
        function updateTranscriptRealtime(speaker, text) {
            const transcriptDiv = document.getElementById('transcript-content');
            const messages = transcriptDiv.querySelectorAll('.transcript-message');
            const lastMessage = messages[messages.length - 1];
            
            // If the last message is from the same speaker, update it
            if (lastMessage && lastMessage.querySelector('.speaker').textContent === speaker + ':') {
                lastMessage.querySelector('.message').textContent = text;
            } else {
                // Otherwise, add a new message
                addToTranscript(speaker, text);
            }
        }

        // Audio chunk queue management for sequenced playback
        let audioChunkQueue = [];
        let currentlyPlaying = false;
        let nextExpectedChunkIndex = 1;
        let isAudioFinished = false;
        
        function queueAudioChunk(chunkData) {
            console.log(`üéµ Queueing audio chunk ${chunkData.chunkIndex}, queue length: ${audioChunkQueue.length}`);
            
            // Add to queue
            audioChunkQueue.push(chunkData);
            
            // Sort queue by chunk index to ensure proper order
            audioChunkQueue.sort((a, b) => a.chunkIndex - b.chunkIndex);
            
            // Try to play next chunk if ready
            processAudioQueue();
        }
        
        function processAudioQueue() {
            // If already playing, wait for current chunk to finish
            if (currentlyPlaying) {
                console.log(`üéµ Currently playing, will process queue after current chunk`);
                return;
            }
            
            // Find the next chunk we're expecting
            const nextChunk = audioChunkQueue.find(chunk => chunk.chunkIndex === nextExpectedChunkIndex);
            
            if (nextChunk) {
                console.log(`üéµ Playing chunk ${nextChunk.chunkIndex}: "${nextChunk.text ? nextChunk.text.substring(0, 30) : 'N/A'}..."`);
                
                // Remove from queue
                audioChunkQueue = audioChunkQueue.filter(chunk => chunk.chunkIndex !== nextChunk.chunkIndex);
                
                // Play this chunk
                currentlyPlaying = true;
                nextExpectedChunkIndex++;
                
                playAudioChunk(nextChunk.audio, () => {
                    // Callback when chunk finishes playing
                    currentlyPlaying = false;
                    
                    // Mark as finished if this was the final chunk
                    if (nextChunk.isFinal) {
                        isAudioFinished = true;
                        console.log(`üéµ Audio playback completed with final chunk ${nextChunk.chunkIndex}`);
                        updateStatus('üé§ Listening for voice input...');
                    }
                    
                    // Process next chunk if available
                    if (!isAudioFinished) {
                        setTimeout(processAudioQueue, 50); // Small delay between chunks
                    }
                });
                
            } else {
                // No next chunk ready yet
                console.log(`üéµ Waiting for chunk ${nextExpectedChunkIndex}, have chunks: ${audioChunkQueue.map(c => c.chunkIndex).join(', ')}`);
            }
        }
        
        function resetAudioQueue() {
            audioChunkQueue = [];
            currentlyPlaying = false;
            nextExpectedChunkIndex = 1;
            isAudioFinished = false;
            console.log(`üéµ Audio queue reset`);
        }

        // Helper function to play audio chunks
        function playAudioChunk(base64Audio, onComplete = null) {
            try {
                console.log('üîä Playing audio chunk, base64 length:', base64Audio.length);
                
                // Convert base64 to blob
                const byteCharacters = atob(base64Audio);
                const byteNumbers = new Array(byteCharacters.length);
                for (let i = 0; i < byteCharacters.length; i++) {
                    byteNumbers[i] = byteCharacters.charCodeAt(i);
                }
                const byteArray = new Uint8Array(byteNumbers);
                
                // Try both WAV and MP3 formats (local TTS uses WAV, ElevenLabs uses MP3)
                let audioBlob;
                if (base64Audio.length > 100) {
                    // Check if it's a WAV file (starts with "RIFF")
                    const firstFourBytes = byteArray.slice(0, 4);
                    const isWav = String.fromCharCode(...firstFourBytes) === 'RIFF';
                    
                    if (isWav) {
                        console.log('üîä Detected WAV format');
                        audioBlob = new Blob([byteArray], { type: 'audio/wav' });
                    } else {
                        console.log('üîä Assuming MP3 format');
                        audioBlob = new Blob([byteArray], { type: 'audio/mpeg' });
                    }
                } else {
                    // Default to MP3 for small chunks
                    audioBlob = new Blob([byteArray], { type: 'audio/mpeg' });
                }
                
                // Create audio URL and play
                const audioUrl = URL.createObjectURL(audioBlob);
                const audio = new Audio(audioUrl);
                
                audio.onloadstart = () => {
                    console.log('üîä Audio loading started');
                };
                
                audio.oncanplay = () => {
                    console.log('üîä Audio can play');
                };
                
                audio.onplay = () => {
                    console.log('üîä Audio playing');
                    updateStatus('üîä Playing AI response...');
                };
                
                audio.onended = () => {
                    console.log('üîä Audio playback finished');
                    URL.revokeObjectURL(audioUrl);
                    // Only update status to listening if this is not part of a sequence
                    if (!onComplete) {
                        updateStatus('üé§ Listening for voice input...');
                    }
                    // Call completion callback if provided
                    if (onComplete && typeof onComplete === 'function') {
                        onComplete();
                    }
                };
                
                audio.onerror = (e) => {
                    console.error('üîä Audio playback error:', e);
                    console.error('üîä Audio error details:', audio.error);
                    URL.revokeObjectURL(audioUrl);
                    updateStatus('‚ùå Audio playback failed');
                    // Call completion callback even on error
                    if (onComplete && typeof onComplete === 'function') {
                        onComplete();
                    }
                };
                
                // Attempt to play with better error handling
                audio.play().then(() => {
                    console.log('üîä Audio play() succeeded');
                }).catch(e => {
                    console.error('üîä Failed to play audio chunk:', e);
                    updateStatus('‚ùå Failed to play audio - click to enable audio');
                });
                
            } catch (error) {
                console.error('üîä Error processing audio chunk:', error);
                updateStatus('‚ùå Error processing audio');
            }
        }

        // Real-time conversation is now handled by sendAudioToChat()
        // No need for simulated responses - using actual AI!

        // Transcript functionality
        function toggleTranscript() {
            const panel = document.getElementById('transcript-panel');
            panel.classList.toggle('hidden');
        }

        function addToTranscript(speaker, text) {
            const content = document.getElementById('transcript-content');
            const entry = document.createElement('div');
            entry.innerHTML = `<strong>${speaker}:</strong> ${text}`;
            entry.style.marginBottom = '10px';
            content.appendChild(entry);
            content.scrollTop = content.scrollHeight;
        }

        // Stop all audio and recording
        function stopAll() {
            // Stop any currently playing audio
            if (currentAudio) {
                currentAudio.pause();
                currentAudio.currentTime = 0;
            }
            if ('speechSynthesis' in window) {
                speechSynthesis.cancel();
            }
            
            // Stop voice sample recording if active
            if (voiceConfig.chatterbox.recording.isRecording) {
                stopRecording();
            }
            
            // Stop STT recording if active
            if (isListening) {
                stopListening();
            }
            
            updateStatus('‚èπÔ∏è All audio stopped');
        }

        // Update status display
        function updateStatus(message) {
            document.getElementById('current-status').textContent = message;
            console.log('Status:', message);
        }

        // Add some CSS for dynamic effects
        const style = document.createElement('style');
        style.textContent = `
            .mic-btn.listening {
                background: #f44336 !important;
                animation: pulse 1s infinite;
            }
            .transcript-panel.hidden {
                display: none;
            }
            .transcript-panel {
                position: fixed;
                top: 50px;
                right: 20px;
                width: 300px;
                height: 400px;
                background: white;
                border: 2px solid #4CAF50;
                border-radius: 10px;
                padding: 20px;
                box-shadow: 0 4px 20px rgba(0,0,0,0.3);
                z-index: 1000;
                overflow-y: auto;
            }
        `;
        document.head.appendChild(style);

        // Initialize with a welcome message (commented out to prevent conflicts with conversation audio)
        // setTimeout(() => {
        //     if (voiceConfig.enabled) {
        //         const engine = voiceConfig.selectedEngine;
        //         if (engine === 'chatterbox') {
        //             synthesizeVoice("Hello! I'm your AI voice assistant with dual voice engines. Currently using Chatterbox TTS for high-quality neural synthesis with emotion control and voice cloning. Click the microphone to start a conversation, or switch to Edge TTS for faster responses!");
        //         } else {
        //             synthesizeVoice("Hello! I'm your AI voice assistant using Edge TTS for fast, real-time conversation. I have hundreds of neural voices available in multiple languages. Click the microphone to start chatting, or switch to Chatterbox for advanced features like voice cloning!");
        //         }
        //     }
        // }, 2000);
    </script>
</body>
</html> 