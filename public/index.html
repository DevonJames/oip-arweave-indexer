<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Voice Assistant - Chatterbox TTS</title>
    <link rel="stylesheet" href="css/style.css">
    <style>
        .chatterbox-controls {
            background: rgba(0, 0, 0, 0.1);
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            border: 2px solid #4CAF50;
        }
        .chatterbox-title {
            color: #4CAF50;
            font-weight: bold;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
        }
        .voice-controls {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 15px;
            margin-bottom: 15px;
        }
        .control-group {
            display: flex;
            flex-direction: column;
        }
        .control-group label {
            margin-bottom: 5px;
            font-weight: bold;
            color: #333;
        }
        .slider-container {
            display: flex;
            align-items: center;
            gap: 10px;
        }
        .slider {
            flex: 1;
            height: 6px;
            border-radius: 3px;
            background: #ddd;
            outline: none;
        }
        .slider::-webkit-slider-thumb {
            appearance: none;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: #4CAF50;
            cursor: pointer;
        }
        .status-indicator {
            display: inline-block;
            width: 10px;
            height: 10px;
            border-radius: 50%;
            margin-right: 8px;
            background: #4CAF50;
            animation: pulse 2s infinite;
        }
        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.5; }
            100% { opacity: 1; }
        }
        .voice-preview {
            background: #f5f5f5;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 10px;
            margin-top: 10px;
            font-style: italic;
            color: #666;
        }
        .recording-active {
            animation: recordingPulse 1s infinite;
        }
        @keyframes recordingPulse {
            0% { background-color: #f44336; }
            50% { background-color: #d32f2f; }
            100% { background-color: #f44336; }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üé≠ AI Voice Assistant - Chatterbox TTS</h1>
            <p>Powered by Resemble AI's neural TTS with emotion control</p>
        </header>
        
        <main>
            <!-- Voice Engine Controls -->
            <div class="chatterbox-controls">
                <h3 class="chatterbox-title">
                    <span class="status-indicator"></span>
                    <span id="engine-title">Voice Engine</span>
                </h3>
                
                <div class="voice-controls">
                    <div class="control-group">
                        <label for="voice-engine">Voice Engine:</label>
                        <select id="voice-engine" onchange="switchEngine()">
                            <option value="chatterbox" selected>Chatterbox TTS (High Quality + Voice Cloning)</option>
                            <option value="edge_tts">Edge TTS (Fast + Many Voices)</option>
                        </select>
                    </div>
                    
                    <!-- Dynamic voice selection based on engine -->
                    <div class="control-group" id="voice-selection-group">
                        <label for="voice-select" id="voice-select-label">Voice:</label>
                        <select id="voice-select" onchange="updateVoiceSelection()">
                            <option value="loading">Loading voices...</option>
                        </select>
                    </div>
                    
                    <div class="control-group">
                        <label>
                            <input type="checkbox" id="tts-enabled" checked onchange="toggleTTS()">
                            <span id="tts-enable-label">Enable Voice Synthesis</span>
                        </label>
                    </div>
                    
                    <!-- Chatterbox-specific controls -->
                    <div id="chatterbox-controls" style="display: block;">
                        <div class="control-group">
                            <label for="emotion-slider">Emotion Level: <span id="emotion-value">70%</span></label>
                            <div class="slider-container">
                                <span>üòê</span>
                                <input type="range" id="emotion-slider" class="slider" min="0" max="100" value="70" oninput="updateEmotion(this.value)">
                                <span>üé≠</span>
                            </div>
                        </div>
                        
                        <div class="control-group">
                            <label for="pacing-slider">Speech Pacing: <span id="pacing-value">40%</span></label>
                            <div class="slider-container">
                                <span>üêå</span>
                                <input type="range" id="pacing-slider" class="slider" min="30" max="70" value="40" oninput="updatePacing(this.value)">
                                <span>üöÄ</span>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Edge TTS-specific controls -->
                    <div id="edge-controls" style="display: none;">
                        <div class="control-group">
                            <label for="edge-speed">Speech Speed: <span id="speed-value">100%</span></label>
                            <div class="slider-container">
                                <span>üêå</span>
                                <input type="range" id="edge-speed" class="slider" min="50" max="200" value="100" oninput="updateSpeed(this.value)">
                                <span>üöÄ</span>
                            </div>
                        </div>
                        
                        <div class="control-group">
                            <label for="edge-pitch">Pitch: <span id="pitch-value">0Hz</span></label>
                            <div class="slider-container">
                                <span>üîΩ</span>
                                <input type="range" id="edge-pitch" class="slider" min="-10" max="10" value="0" oninput="updatePitch(this.value)">
                                <span>üîº</span>
                            </div>
                        </div>
                        
                        <div class="control-group">
                            <label for="edge-volume">Volume: <span id="volume-value">0%</span></label>
                            <div class="slider-container">
                                <span>üîâ</span>
                                <input type="range" id="edge-volume" class="slider" min="-20" max="20" value="0" oninput="updateVolume(this.value)">
                                <span>üîä</span>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Voice Cloning Section (Chatterbox only) -->
                    <div id="voice-cloning-section" style="display: block;">
                    <div style="margin-top: 15px; border-top: 2px solid #4CAF50; padding-top: 15px;">
                        <h4 style="color: #4CAF50; margin-bottom: 10px;">üé§ Voice Cloning (Zero-Shot)</h4>
                        
                        <div class="control-group">
                            <label>Voice Sample Options:</label>
                            <div style="display: flex; gap: 10px; margin-bottom: 10px;">
                                <button type="button" id="record-sample-btn" class="control-btn" onclick="toggleRecording()" style="flex: 1; padding: 8px;">
                                    <span id="record-btn-text">üéôÔ∏è Record Sample</span>
                                </button>
                                <button type="button" onclick="document.getElementById('voice-sample-upload').click()" class="control-btn" style="flex: 1; padding: 8px;">
                                    üìÅ Upload File
                                </button>
                            </div>
                            <input type="file" id="voice-sample-upload" accept="audio/*" onchange="handleVoiceSampleUpload(this)" style="display: none;">
                            <div id="recording-controls" style="display: none; background: #f9f9f9; padding: 10px; border-radius: 5px; margin-bottom: 10px;">
                                <div style="display: flex; align-items: center; gap: 10px;">
                                    <span id="recording-timer">00:00</span>
                                    <div style="flex: 1; background: #ddd; height: 4px; border-radius: 2px;">
                                        <div id="recording-progress" style="background: #f44336; height: 100%; width: 0%; border-radius: 2px; transition: width 0.1s;"></div>
                                    </div>
                                    <span style="font-size: 12px; color: #666;">10s max</span>
                                </div>
                                <small style="color: #666; display: block; margin-top: 5px;">
                                    üí° <strong>Speak clearly for 3-10 seconds.</strong> Say anything: your name, a sentence, count to ten, or read some text aloud.
                                </small>
                            </div>
                            <small style="color: #666;">
                                Record a 3-10 second voice sample or upload an audio file (WAV/MP3). Content doesn't matter - just speak clearly!
                            </small>
                        </div>
                        
                        <div class="control-group">
                            <label>
                                <input type="checkbox" id="voice-cloning-enabled" onchange="toggleVoiceCloning()">
                                Enable Voice Cloning Mode
                            </label>
                        </div>
                        
                        <div id="voice-clone-status" style="padding: 8px; background: #f0f0f0; border-radius: 5px; font-size: 12px; color: #666;">
                            No voice sample uploaded. Using default Chatterbox voices.
                        </div>
                    </div>
                    </div> <!-- End voice-cloning-section -->
                </div>
                
                <div class="voice-preview" id="voice-preview">
                    Current: Female expressive voice with 70% emotion and 30% pacing speed. Engaging female voice with natural expressiveness. ‚ú® Optimized for engaging, natural speech.
                </div>
            </div>
            
            <!-- Voice Assistant Interface -->
            <div id="voice-assistant-root">
                <div class="waveform-container">
                    <canvas id="waveform"></canvas>
                </div>
                
                <div class="controls">
                    <button id="microphone-btn" class="mic-btn" onclick="toggleMicrophone()">
                        <span class="mic-icon">üéôÔ∏è</span>
                        <span id="mic-status">Click to Start</span>
                    </button>
                    <button id="test-voice-btn" class="control-btn" onclick="testChatterboxVoice()">
                        <span>üéµ Test Voice</span>
                    </button>
                    <button id="test-clone-btn" class="control-btn" onclick="testVoiceCloning()" style="display: none;">
                        <span>üé§ Test Clone</span>
                    </button>
                    <button id="transcript-btn" class="control-btn" onclick="toggleTranscript()">
                        <span>üìÑ Transcript</span>
                    </button>
                    <button id="stop-btn" class="control-btn" onclick="stopAll()">
                        <span>‚èπÔ∏è Stop</span>
                    </button>
                </div>
            </div>
            
            <!-- Status Display -->
            <div id="status-display" style="margin-top: 20px; padding: 10px; background: #f0f0f0; border-radius: 5px;">
                <strong>Status:</strong> <span id="current-status">Chatterbox TTS Ready</span>
            </div>
        </main>
        
        <div id="transcript-panel" class="transcript-panel hidden">
            <div class="transcript-header">
                <h2>Conversation Transcript</h2>
                <button id="close-transcript" class="close-btn" onclick="toggleTranscript()">‚úï</button>
            </div>
            <div id="transcript-content" class="transcript-content"></div>
        </div>
    </div>

    <script>
        // Voice Engine Configuration
        const voiceConfig = {
            enabled: true,
            selectedEngine: 'chatterbox',
            baseUrl: window.location.origin,
            // Chatterbox-specific settings
            chatterbox: {
                selectedVoice: 'female_expressive',
                exaggeration: 0.7,       // 70% emotion for engaging speech
                cfg_weight: 0.3,         // Lower weight for better pacing with expressive tone
                voiceCloning: {
                    enabled: false,
                    audioFile: null,
                    fileName: null
                },
                recording: {
                    isRecording: false,
                    mediaRecorder: null,
                    audioChunks: [],
                    startTime: null,
                    maxDuration: 10000, // 10 seconds
                    timer: null
                }
            },
            // Edge TTS-specific settings
            edge: {
                selectedVoice: 'en-US-AriaNeural',
                speed: 1.0,              // Normal speed
                pitch: 0,                // No pitch adjustment
                volume: 0                // No volume adjustment
            }
        };
        
        // Available voices for each engine (loaded dynamically)
        let availableVoices = {
            chatterbox: [],
            edge_tts: []
        };

        // Voice configuration matrix: Gender + Emotion combinations
        const voiceMatrix = {
            'female': {
                'expressive': { exaggeration: 0.7, cfg_weight: 0.3, description: 'Engaging female voice with natural expressiveness' },
                'calm': { exaggeration: 0.2, cfg_weight: 0.6, description: 'Soothing female voice with measured pacing' },
                'dramatic': { exaggeration: 0.9, cfg_weight: 0.2, description: 'Intense female voice with maximum emotion' },
                'neutral': { exaggeration: 0.5, cfg_weight: 0.5, description: 'Balanced female voice for general use' }
            },
            'male': {
                'expressive': { exaggeration: 0.6, cfg_weight: 0.3, description: 'Dynamic male voice with controlled emotion' },
                'calm': { exaggeration: 0.3, cfg_weight: 0.6, description: 'Deep, calming male voice with slow delivery' },
                'dramatic': { exaggeration: 0.8, cfg_weight: 0.2, description: 'Powerful male voice with commanding presence' },
                'neutral': { exaggeration: 0.4, cfg_weight: 0.5, description: 'Professional male voice for clear communication' }
            }
        };

        let isListening = false;
        let currentAudio = null;
        let mediaRecorder = null;
        let audioChunks = [];
        let conversationHistory = [];

        // Initialize Voice Assistant on page load
        document.addEventListener('DOMContentLoaded', function() {
            console.log('üé≠ Initializing Voice Assistant...');
            loadAvailableVoices();
            checkVoiceStatus();
        });
        
        // Switch between voice engines
        async function switchEngine() {
            const selectedEngine = document.getElementById('voice-engine').value;
            voiceConfig.selectedEngine = selectedEngine;
            
            console.log(`üîÑ Switching to engine: ${selectedEngine}`);
            
            // Update UI visibility
            if (selectedEngine === 'chatterbox') {
                document.getElementById('chatterbox-controls').style.display = 'block';
                document.getElementById('edge-controls').style.display = 'none';
                document.getElementById('voice-cloning-section').style.display = 'block';
                document.getElementById('engine-title').textContent = 'Chatterbox TTS (High Quality)';
                document.getElementById('tts-enable-label').textContent = 'Enable Chatterbox TTS';
            } else {
                document.getElementById('chatterbox-controls').style.display = 'none';
                document.getElementById('edge-controls').style.display = 'block';
                document.getElementById('voice-cloning-section').style.display = 'none';
                document.getElementById('engine-title').textContent = 'Edge TTS (Fast Response)';
                document.getElementById('tts-enable-label').textContent = 'Enable Edge TTS';
            }
            
            // Load voices for selected engine
            await populateVoiceDropdown(selectedEngine);
            updateVoicePreview();
            updateStatus(`üîÑ Switched to ${selectedEngine === 'chatterbox' ? 'Chatterbox TTS' : 'Edge TTS'}`);
        }
        
        // Load available voices from API
        async function loadAvailableVoices() {
            try {
                // Load Chatterbox voices
                const chatterboxResponse = await fetch('/api/voice/voices/chatterbox');
                if (chatterboxResponse.ok) {
                    const chatterboxData = await chatterboxResponse.json();
                    availableVoices.chatterbox = chatterboxData.voices || [];
                }
                
                // Load Edge TTS voices
                const edgeResponse = await fetch('/api/voice/voices/edge_tts');
                if (edgeResponse.ok) {
                    const edgeData = await edgeResponse.json();
                    availableVoices.edge_tts = edgeData.voices || [];
                }
                
                console.log(`üéµ Loaded ${availableVoices.chatterbox.length} Chatterbox voices and ${availableVoices.edge_tts.length} Edge TTS voices`);
                
                // Populate dropdown for default engine
                await populateVoiceDropdown(voiceConfig.selectedEngine);
                
            } catch (error) {
                console.error('Failed to load voices:', error);
                updateStatus('‚ö†Ô∏è Failed to load voice options');
            }
        }
        
        // Populate voice dropdown based on selected engine
        async function populateVoiceDropdown(engine) {
            const voiceSelect = document.getElementById('voice-select');
            voiceSelect.innerHTML = ''; // Clear existing options
            
            const voices = availableVoices[engine] || [];
            
            if (voices.length === 0) {
                voiceSelect.innerHTML = '<option value="">No voices available</option>';
                return;
            }
            
            // Group voices by gender for better organization
            const groupedVoices = {
                female: voices.filter(v => v.gender === 'female'),
                male: voices.filter(v => v.gender === 'male'),
                other: voices.filter(v => !['female', 'male'].includes(v.gender))
            };
            
            // Add female voices
            if (groupedVoices.female.length > 0) {
                const femaleGroup = document.createElement('optgroup');
                femaleGroup.label = 'Female Voices';
                groupedVoices.female.forEach(voice => {
                    const option = document.createElement('option');
                    option.value = voice.id;
                    option.textContent = voice.name;
                    option.title = voice.description || '';
                    femaleGroup.appendChild(option);
                });
                voiceSelect.appendChild(femaleGroup);
            }
            
            // Add male voices
            if (groupedVoices.male.length > 0) {
                const maleGroup = document.createElement('optgroup');
                maleGroup.label = 'Male Voices';
                groupedVoices.male.forEach(voice => {
                    const option = document.createElement('option');
                    option.value = voice.id;
                    option.textContent = voice.name;
                    option.title = voice.description || '';
                    maleGroup.appendChild(option);
                });
                voiceSelect.appendChild(maleGroup);
            }
            
            // Add other voices
            if (groupedVoices.other.length > 0) {
                const otherGroup = document.createElement('optgroup');
                otherGroup.label = 'Other Voices';
                groupedVoices.other.forEach(voice => {
                    const option = document.createElement('option');
                    option.value = voice.id;
                    option.textContent = voice.name;
                    option.title = voice.description || '';
                    otherGroup.appendChild(option);
                });
                voiceSelect.appendChild(otherGroup);
            }
            
            // Set default selection
            if (engine === 'chatterbox') {
                const defaultVoice = voices.find(v => v.id === 'female_expressive') || voices[0];
                if (defaultVoice) {
                    voiceSelect.value = defaultVoice.id;
                    voiceConfig.chatterbox.selectedVoice = defaultVoice.id;
                }
            } else if (engine === 'edge_tts') {
                const defaultVoice = voices.find(v => v.id === 'en-US-AriaNeural') || voices[0];
                if (defaultVoice) {
                    voiceSelect.value = defaultVoice.id;
                    voiceConfig.edge.selectedVoice = defaultVoice.id;
                }
            }
            
            console.log(`üéµ Populated ${voices.length} voices for ${engine}`);
        }
        
        // Handle voice selection change
        function updateVoiceSelection() {
            const selectedVoice = document.getElementById('voice-select').value;
            const engine = voiceConfig.selectedEngine;
            
            if (engine === 'chatterbox') {
                voiceConfig.chatterbox.selectedVoice = selectedVoice;
            } else if (engine === 'edge_tts') {
                voiceConfig.edge.selectedVoice = selectedVoice;
            }
            
            updateVoicePreview();
            console.log(`üéµ Selected voice: ${selectedVoice} for engine: ${engine}`);
        }

        // Check voice service status
        async function checkVoiceStatus() {
            try {
                const response = await fetch('/api/voice/health');
                const data = await response.json();
                
                const engines = data.engines || [];
                const chatterboxAvailable = engines.find(e => e.name === 'chatterbox')?.available || false;
                const edgeAvailable = engines.find(e => e.name === 'edge_tts')?.available || false;
                
                if (chatterboxAvailable && edgeAvailable) {
                    updateStatus('‚úÖ All Voice Engines Ready - Chatterbox & Edge TTS Available');
                    document.querySelector('.status-indicator').style.background = '#4CAF50';
                } else if (chatterboxAvailable || edgeAvailable) {
                    updateStatus('‚ö†Ô∏è Some Voice Engines Available - Check Engine Selection');
                    document.querySelector('.status-indicator').style.background = '#FF9800';
                } else {
                    updateStatus('‚ùå No Voice Engines Available');
                    document.querySelector('.status-indicator').style.background = '#f44336';
                }
            } catch (error) {
                console.error('Failed to check voice status:', error);
                updateStatus('‚ùå Voice Service Connection Error');
                document.querySelector('.status-indicator').style.background = '#f44336';
            }
        }
        
        // Edge TTS slider update functions
        function updateSpeed(value) {
            voiceConfig.edge.speed = value / 100;
            document.getElementById('speed-value').textContent = value + '%';
            updateVoicePreview();
        }
        
        function updatePitch(value) {
            voiceConfig.edge.pitch = parseInt(value);
            document.getElementById('pitch-value').textContent = value + 'Hz';
            updateVoicePreview();
        }
        
        function updateVolume(value) {
            voiceConfig.edge.volume = parseInt(value);
            document.getElementById('volume-value').textContent = value + '%';
            updateVoicePreview();
        }

        // Update emotion level (Chatterbox)
        function updateEmotion(value) {
            voiceConfig.chatterbox.exaggeration = value / 100;
            document.getElementById('emotion-value').textContent = value + '%';
            updateVoicePreview();
        }

        // Update pacing (Chatterbox)
        function updatePacing(value) {
            voiceConfig.chatterbox.cfg_weight = value / 100;
            document.getElementById('pacing-value').textContent = value + '%';
            updateVoicePreview();
        }

        // Toggle TTS on/off
        function toggleTTS() {
            voiceConfig.enabled = document.getElementById('tts-enabled').checked;
            const engineName = voiceConfig.selectedEngine === 'chatterbox' ? 'Chatterbox TTS' : 'Edge TTS';
            updateStatus(voiceConfig.enabled ? `${engineName} Enabled` : 'TTS Disabled');
            updateVoicePreview();
        }

        // Handle voice sample file upload
        function handleVoiceSampleUpload(input) {
            const file = input.files[0];
            if (!file) {
                voiceConfig.chatterbox.voiceCloning.audioFile = null;
                voiceConfig.chatterbox.voiceCloning.fileName = null;
                updateVoiceCloneStatus('No voice sample uploaded. Using default Chatterbox voices.');
                return;
            }

            // Validate file type
            const validTypes = ['audio/wav', 'audio/mpeg', 'audio/mp3', 'audio/ogg', 'audio/webm'];
            if (!validTypes.includes(file.type)) {
                alert('Please upload a valid audio file (WAV, MP3, OGG, or WebM)');
                input.value = '';
                return;
            }

            // Validate file size (max 10MB)
            if (file.size > 10 * 1024 * 1024) {
                alert('File size must be less than 10MB');
                input.value = '';
                return;
            }

            voiceConfig.chatterbox.voiceCloning.audioFile = file;
            voiceConfig.chatterbox.voiceCloning.fileName = file.name;
            
            updateVoiceCloneStatus(`Voice sample loaded: ${file.name} (${(file.size / 1024 / 1024).toFixed(2)}MB). Enable voice cloning to use it.`);
            
            console.log('üé§ Voice sample uploaded:', file.name, file.type);
        }

        // Toggle voice cloning mode
        function toggleVoiceCloning() {
            const enabled = document.getElementById('voice-cloning-enabled').checked;
            
            if (enabled && !voiceConfig.chatterbox.voiceCloning.audioFile) {
                alert('Please upload a voice sample first!');
                document.getElementById('voice-cloning-enabled').checked = false;
                return;
            }
            
            voiceConfig.chatterbox.voiceCloning.enabled = enabled;
            
            // Show/hide voice cloning test button
            const testCloneBtn = document.getElementById('test-clone-btn');
            if (enabled) {
                testCloneBtn.style.display = 'inline-block';
                updateVoiceCloneStatus(`‚úÖ Voice cloning active with ${voiceConfig.chatterbox.voiceCloning.fileName}. Your AI will speak in the uploaded voice!`);
                updateStatus('üé§ Voice cloning mode enabled - AI will use your uploaded voice sample');
            } else {
                testCloneBtn.style.display = 'none';
                updateVoiceCloneStatus(`Voice sample available: ${voiceConfig.chatterbox.voiceCloning.fileName}. Enable voice cloning to use it.`);
                updateStatus('üé≠ Using default Chatterbox voices');
            }
            
            updateVoicePreview();
        }

        // Test voice cloning specifically
        async function testVoiceCloning() {
            if (!voiceConfig.chatterbox.voiceCloning.enabled || !voiceConfig.chatterbox.voiceCloning.audioFile) {
                updateStatus('‚ùå Voice cloning not enabled or no audio file uploaded');
                return;
            }

            const testText = `Hello! This is a test of voice cloning using your uploaded voice sample. I should now sound like the person in your audio file, with ${Math.round(voiceConfig.chatterbox.exaggeration * 100)} percent emotional expression.`;
            
            updateStatus('üé§ Testing voice cloning...');
            
            try {
                await synthesizeVoice(testText);
                updateStatus('‚úÖ Voice cloning test completed successfully!');
            } catch (error) {
                console.error('Voice cloning test failed:', error);
                updateStatus('‚ùå Voice cloning test failed - check console for details');
            }
        }

        // Update voice clone status display
        function updateVoiceCloneStatus(message) {
            document.getElementById('voice-clone-status').textContent = message;
        }

        // Toggle microphone recording for voice sample
        async function toggleRecording() {
            if (voiceConfig.chatterbox.recording.isRecording) {
                stopRecording();
            } else {
                await startRecording();
            }
        }

        // Start recording voice sample
        async function startRecording() {
            try {
                // Request microphone access
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        sampleRate: 44100,
                        channelCount: 1,
                        volume: 1.0,
                        echoCancellation: true,
                        noiseSuppression: true
                    } 
                });

                // Set up MediaRecorder
                voiceConfig.chatterbox.recording.mediaRecorder = new MediaRecorder(stream);
                voiceConfig.chatterbox.recording.audioChunks = [];
                voiceConfig.chatterbox.recording.startTime = Date.now();

                // Update UI for recording state
                voiceConfig.chatterbox.recording.isRecording = true;
                document.getElementById('record-btn-text').textContent = '‚èπÔ∏è Stop Recording';
                document.getElementById('record-sample-btn').classList.add('recording-active');
                document.getElementById('recording-controls').style.display = 'block';

                // Start timer and progress
                updateRecordingTimer();

                // Handle recording data
                voiceConfig.chatterbox.recording.mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        voiceConfig.chatterbox.recording.audioChunks.push(event.data);
                    }
                };

                // Handle recording stop
                voiceConfig.chatterbox.recording.mediaRecorder.onstop = () => {
                    const audioBlob = new Blob(voiceConfig.chatterbox.recording.audioChunks, { 
                        type: 'audio/wav' 
                    });
                    
                    // Create a File object from the blob
                    const audioFile = new File([audioBlob], `voice_sample_${Date.now()}.wav`, {
                        type: 'audio/wav'
                    });
                    
                    // Use the recorded audio as voice sample
                    voiceConfig.chatterbox.voiceCloning.audioFile = audioFile;
                    voiceConfig.chatterbox.voiceCloning.fileName = audioFile.name;
                    
                    const duration = ((Date.now() - voiceConfig.chatterbox.recording.startTime) / 1000).toFixed(1);
                    updateVoiceCloneStatus(`Voice sample recorded: ${duration}s audio. Enable voice cloning to use it.`);
                    
                    // Stop all tracks to release microphone
                    stream.getTracks().forEach(track => track.stop());
                    
                    console.log('üé§ Voice sample recorded:', audioFile.name, `${duration}s`);
                };

                // Start recording
                voiceConfig.chatterbox.recording.mediaRecorder.start();
                updateStatus('üé§ Recording voice sample... Speak clearly for 3-10 seconds');

                // Auto-stop after max duration
                setTimeout(() => {
                    if (voiceConfig.chatterbox.recording.isRecording) {
                        stopRecording();
                    }
                }, voiceConfig.chatterbox.recording.maxDuration);

            } catch (error) {
                console.error('Failed to start recording:', error);
                updateStatus('‚ùå Microphone access denied or not available');
                alert('Could not access microphone. Please check permissions and try again.');
            }
        }

        // Stop recording voice sample
        function stopRecording() {
            if (!voiceConfig.chatterbox.recording.isRecording) return;

            voiceConfig.chatterbox.recording.isRecording = false;
            
            // Stop MediaRecorder
            if (voiceConfig.chatterbox.recording.mediaRecorder && 
                voiceConfig.chatterbox.recording.mediaRecorder.state !== 'inactive') {
                voiceConfig.chatterbox.recording.mediaRecorder.stop();
            }

            // Clear timer
            if (voiceConfig.chatterbox.recording.timer) {
                clearInterval(voiceConfig.chatterbox.recording.timer);
            }

            // Reset UI
            document.getElementById('record-btn-text').textContent = 'üéôÔ∏è Record Sample';
            document.getElementById('record-sample-btn').classList.remove('recording-active');
            document.getElementById('recording-controls').style.display = 'none';
            document.getElementById('recording-timer').textContent = '00:00';
            document.getElementById('recording-progress').style.width = '0%';

            updateStatus('üéµ Recording completed - voice sample ready for cloning');
        }

        // Update recording timer and progress
        function updateRecordingTimer() {
            voiceConfig.chatterbox.recording.timer = setInterval(() => {
                if (!voiceConfig.chatterbox.recording.isRecording) return;

                const elapsed = Date.now() - voiceConfig.chatterbox.recording.startTime;
                const seconds = Math.floor(elapsed / 1000);
                const progress = Math.min((elapsed / voiceConfig.chatterbox.recording.maxDuration) * 100, 100);

                // Update timer display
                const minutes = Math.floor(seconds / 60);
                const secs = seconds % 60;
                document.getElementById('recording-timer').textContent = 
                    `${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;

                // Update progress bar
                document.getElementById('recording-progress').style.width = `${progress}%`;

                // Auto-stop if max duration reached
                if (elapsed >= voiceConfig.chatterbox.recording.maxDuration) {
                    stopRecording();
                }
            }, 100);
        }

        // Update voice preview description
        function updateVoicePreview() {
            if (!voiceConfig.enabled) {
                document.getElementById('voice-preview').textContent = '‚ö†Ô∏è Voice synthesis is currently disabled.';
                return;
            }
            
            const engine = voiceConfig.selectedEngine;
            let preview = '';
            
            if (engine === 'chatterbox') {
                const emotionLevel = Math.round(voiceConfig.chatterbox.exaggeration * 100);
                const pacing = Math.round(voiceConfig.chatterbox.cfg_weight * 100);
                const selectedVoice = voiceConfig.chatterbox.selectedVoice;
                
                // Voice cloning mode
                if (voiceConfig.chatterbox.voiceCloning.enabled && voiceConfig.chatterbox.voiceCloning.fileName) {
                    preview = `üé§ Voice Cloning Active: Using uploaded voice sample "${voiceConfig.chatterbox.voiceCloning.fileName}" with ${emotionLevel}% emotion and ${pacing}% pacing. The AI will speak in your custom voice!`;
                } else {
                    // Get voice info from available voices
                    const voiceInfo = availableVoices.chatterbox.find(v => v.id === selectedVoice);
                    const voiceName = voiceInfo ? voiceInfo.name : selectedVoice;
                    
                    preview = `Current: ${voiceName} with ${emotionLevel}% emotion and ${pacing}% pacing. High-quality neural synthesis with advanced emotion control. ‚ú® Optimized for engaging, natural speech.`;
                }
            } else if (engine === 'edge_tts') {
                const speed = Math.round(voiceConfig.edge.speed * 100);
                const pitch = voiceConfig.edge.pitch;
                const volume = voiceConfig.edge.volume;
                const selectedVoice = voiceConfig.edge.selectedVoice;
                
                // Get voice info from available voices
                const voiceInfo = availableVoices.edge_tts.find(v => v.id === selectedVoice);
                const voiceName = voiceInfo ? voiceInfo.name : selectedVoice;
                
                preview = `Current: ${voiceName} at ${speed}% speed`;
                if (pitch !== 0) preview += `, ${pitch > 0 ? '+' : ''}${pitch}Hz pitch`;
                if (volume !== 0) preview += `, ${volume > 0 ? '+' : ''}${volume}% volume`;
                preview += '. Fast response neural voice perfect for real-time conversation. ‚ö° Instant synthesis.';
            }
            
            document.getElementById('voice-preview').textContent = preview;
        }
        
        // Universal voice synthesis function (handles both engines)
        async function synthesizeVoice(text) {
            if (!voiceConfig.enabled) {
                console.log('Voice synthesis disabled, skipping');
                return;
            }

            const engine = voiceConfig.selectedEngine;
            
            try {
                if (engine === 'chatterbox') {
                    await synthesizeWithChatterbox(text);
                } else if (engine === 'edge_tts') {
                    await synthesizeWithEdge(text);
                } else {
                    throw new Error(`Unknown engine: ${engine}`);
                }
            } catch (error) {
                console.error(`Voice synthesis failed with ${engine}:`, error);
                updateStatus(`‚ùå ${engine} synthesis failed - trying fallback...`);
                
                // Fallback to browser TTS if both engines fail
                if ('speechSynthesis' in window) {
                    const utterance = new SpeechSynthesisUtterance(text);
                    utterance.rate = 0.9;
                    utterance.pitch = 1.0;
                    utterance.volume = 0.8;
                    speechSynthesis.speak(utterance);
                    updateStatus('üîä Using browser TTS fallback');
                }
            }
        }
        
        // Synthesize with Chatterbox TTS (existing function updated)
        async function synthesizeWithChatterbox(text) {
            try {
                const isCloning = voiceConfig.chatterbox.voiceCloning.enabled;
                updateStatus(isCloning ? 'üé§ Synthesizing with voice cloning...' : 'üéµ Synthesizing with Chatterbox TTS...');
                
                // Always use FormData for consistency with backend expectations
                const formData = new FormData();
                formData.append('text', text);
                formData.append('voice_id', voiceConfig.chatterbox.selectedVoice);
                formData.append('exaggeration', voiceConfig.chatterbox.exaggeration.toString());
                formData.append('cfg_weight', voiceConfig.chatterbox.cfg_weight.toString());
                formData.append('engine', 'chatterbox');
                
                if (isCloning && voiceConfig.chatterbox.voiceCloning.audioFile) {
                    formData.append('voice_cloning', 'true');
                    formData.append('audio_prompt', voiceConfig.chatterbox.voiceCloning.audioFile);
                    console.log('üé§ Adding voice cloning audio file to request:', voiceConfig.chatterbox.voiceCloning.audioFile.name);
                } else {
                    formData.append('voice_cloning', 'false');
                }
                
                // Create abort controller for timeout (2 minutes for Chatterbox synthesis)
                const controller = new AbortController();
                const timeoutId = setTimeout(() => controller.abort(), 120000);
                
                const response = await fetch('/api/voice/synthesize', {
                    method: 'POST',
                    body: formData,
                    signal: controller.signal
                });
                
                clearTimeout(timeoutId);

                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }

                const audioBlob = await response.blob();
                const audioUrl = URL.createObjectURL(audioBlob);
                
                // Stop any currently playing audio and play new audio
                if (currentAudio) {
                    currentAudio.pause();
                    currentAudio.currentTime = 0;
                }
                
                currentAudio = new Audio(audioUrl);
                currentAudio.play();
                
                currentAudio.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    updateStatus('üé≠ Chatterbox TTS Ready');
                };
                
                updateStatus('üîä Playing Chatterbox synthesis...');
                
            } catch (error) {
                console.error('Chatterbox synthesis failed:', error);
                if (error.name === 'AbortError') {
                    updateStatus('‚è±Ô∏è Chatterbox synthesis taking longer than expected...');
                }
                throw error;
            }
        }
        
        // Synthesize with Edge TTS (new function)
        async function synthesizeWithEdge(text) {
            try {
                updateStatus('‚ö° Synthesizing with Edge TTS...');
                
                // Prepare form data for Edge TTS
                const formData = new FormData();
                formData.append('text', text);
                formData.append('voice_id', voiceConfig.edge.selectedVoice);
                formData.append('speed', voiceConfig.edge.speed.toString());
                formData.append('engine', 'edge_tts');
                
                // Add exaggeration and cfg_weight for compatibility (Edge will convert these to pitch/volume)
                formData.append('exaggeration', (voiceConfig.edge.pitch / 10 + 0.5).toString()); // Convert pitch to exaggeration range
                formData.append('cfg_weight', (voiceConfig.edge.volume / 20 + 0.5).toString());  // Convert volume to cfg_weight range
                
                const response = await fetch('/api/voice/synthesize', {
                    method: 'POST',
                    body: formData
                });

                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }

                const audioBlob = await response.blob();
                const audioUrl = URL.createObjectURL(audioBlob);
                
                // Stop any currently playing audio and play new audio
                if (currentAudio) {
                    currentAudio.pause();
                    currentAudio.currentTime = 0;
                }
                
                currentAudio = new Audio(audioUrl);
                currentAudio.play();
                
                currentAudio.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    updateStatus('‚ö° Edge TTS Ready');
                };
                
                updateStatus('üîä Playing Edge TTS synthesis...');
                
            } catch (error) {
                console.error('Edge TTS synthesis failed:', error);
                throw error;
            }
        }

        // Test current voice settings
        async function testChatterboxVoice() {
            if (!voiceConfig.enabled) {
                updateStatus('‚ùå Voice synthesis is disabled. Enable it first.');
                return;
            }

            const engine = voiceConfig.selectedEngine;
            let testText = '';
            
            if (engine === 'chatterbox') {
                const emotionLevel = Math.round(voiceConfig.chatterbox.exaggeration * 100);
                testText = `Hello! I'm your AI assistant powered by Chatterbox TTS. This voice has ${emotionLevel} percent emotion and advanced neural synthesis with voice cloning capabilities.`;
            } else if (engine === 'edge_tts') {
                const speed = Math.round(voiceConfig.edge.speed * 100);
                testText = `Hello! I'm your AI assistant using Edge TTS at ${speed} percent speed. This is perfect for fast, real-time conversation with natural neural voices.`;
            }
            
            updateStatus(`üéµ Testing ${engine === 'chatterbox' ? 'Chatterbox' : 'Edge TTS'} voice...`);
            
            try {
                await synthesizeVoice(testText);
                updateStatus('‚úÖ Voice test completed successfully!');
            } catch (error) {
                console.error('Voice test failed:', error);
                updateStatus('‚ùå Voice test failed - check console for details');
            }
        }

        // Synthesize speech using Chatterbox TTS
        async function synthesizeWithChatterbox(text) {
            if (!voiceConfig.enabled) {
                console.log('TTS disabled, skipping synthesis');
                return;
            }

            try {
                updateStatus(voiceConfig.chatterbox.voiceCloning.enabled ? 
                    'üé§ Synthesizing with voice cloning...' : 
                    'üéµ Synthesizing with Chatterbox TTS...');
                
                // Always use FormData for consistency with backend expectations
                const formData = new FormData();
                formData.append('text', text);
                formData.append('voice_id', voiceConfig.chatterbox.selectedVoice);
                formData.append('exaggeration', voiceConfig.chatterbox.exaggeration.toString());
                formData.append('cfg_weight', voiceConfig.chatterbox.cfg_weight.toString());
                formData.append('engine', 'chatterbox');
                
                if (voiceConfig.chatterbox.voiceCloning.enabled && voiceConfig.chatterbox.voiceCloning.audioFile) {
                    // Voice cloning mode - add audio file
                    formData.append('voice_cloning', 'true');
                    formData.append('audio_prompt', voiceConfig.chatterbox.voiceCloning.audioFile);
                    console.log('üé§ Adding voice cloning audio file to request:', voiceConfig.chatterbox.voiceCloning.audioFile.name);
                } else {
                    // Regular synthesis mode
                    formData.append('voice_cloning', 'false');
                }
                
                const requestBody = formData;
                const headers = {}; // Don't set Content-Type, let browser set it for FormData
                
                // Create abort controller for timeout (2 minutes for Chatterbox synthesis)
                const controller = new AbortController();
                const timeoutId = setTimeout(() => controller.abort(), 120000);
                
                const response = await fetch('/api/voice/synthesize', {
                    method: 'POST',
                    headers: headers,
                    body: requestBody,
                    signal: controller.signal
                });
                
                clearTimeout(timeoutId);

                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }

                const audioBlob = await response.blob();
                const audioUrl = URL.createObjectURL(audioBlob);
                
                // Stop any currently playing audio
                if (currentAudio) {
                    currentAudio.pause();
                    currentAudio.currentTime = 0;
                }
                
                // Play the new audio
                currentAudio = new Audio(audioUrl);
                currentAudio.play();
                
                currentAudio.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    updateStatus('üé≠ Chatterbox TTS Ready');
                };
                
                updateStatus('üîä Playing Chatterbox synthesis...');
                
            } catch (error) {
                console.error('Chatterbox synthesis failed:', error);
                
                // Handle timeout specifically
                if (error.name === 'AbortError') {
                    updateStatus('‚è±Ô∏è Chatterbox synthesis taking longer than expected - trying fallback...');
                } else {
                    updateStatus('‚ùå Synthesis failed - trying fallback...');
                }
                
                // Fallback to browser TTS if Chatterbox fails
                if ('speechSynthesis' in window) {
                    const utterance = new SpeechSynthesisUtterance(text);
                    utterance.rate = 0.9;
                    utterance.pitch = 1.0;
                    utterance.volume = 0.8;
                    speechSynthesis.speak(utterance);
                    updateStatus('üîä Using browser TTS fallback');
                }
            }
        }

        // Real microphone functionality with STT integration
        async function toggleMicrophone() {
            if (isListening) {
                stopListening();
            } else {
                await startListening();
            }
        }

        async function startListening() {
            try {
                // Request microphone access
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        sampleRate: 16000,  // Optimal for Whisper STT  
                        channelCount: 1,
                        volume: 1.0,
                        echoCancellation: true,
                        noiseSuppression: true
                    } 
                });

                // Set up MediaRecorder
                mediaRecorder = new MediaRecorder(stream, {
                    mimeType: 'audio/webm;codecs=opus'
                });
                audioChunks = [];

                // Update UI
                isListening = true;
                const btn = document.getElementById('microphone-btn');
                const status = document.getElementById('mic-status');
                btn.classList.add('listening');
                status.textContent = 'Listening...';
                updateStatus('üé§ Listening for voice input...');

                // Handle recording data
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                // Handle recording stop
                mediaRecorder.onstop = async () => {
                    updateStatus('üîÑ Processing speech...');
                    
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    
                    // Send to chat endpoint for STT + AI response
                    await sendAudioToChat(audioBlob);
                    
                    // Stop all tracks to release microphone
                    stream.getTracks().forEach(track => track.stop());
                };

                // Start recording
                mediaRecorder.start();

                // Auto-stop after 30 seconds max
                setTimeout(() => {
                    if (isListening) {
                        stopListening();
                    }
                }, 30000);

            } catch (error) {
                console.error('Failed to start listening:', error);
                updateStatus('‚ùå Microphone access denied or not available');
                alert('Could not access microphone. Please check permissions and try again.');
            }
        }

        function stopListening() {
            if (!isListening) return;

            isListening = false;
            
            // Stop MediaRecorder
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }

            // Reset UI
            const btn = document.getElementById('microphone-btn');
            const status = document.getElementById('mic-status');
            btn.classList.remove('listening');
            status.textContent = 'Click to Start';
            updateStatus('üîÑ Processing...');
        }

        // Send recorded audio to chat endpoint for STT + AI response
        async function sendAudioToChat(audioBlob) {
            try {
                // Create FormData with audio file
                const formData = new FormData();
                formData.append('audio', audioBlob, 'recording.webm');
                
                // Add conversation history if available
                if (conversationHistory.length > 0) {
                    formData.append('conversationHistory', JSON.stringify(conversationHistory));
                }

                // Send to chat endpoint
                const response = await fetch('/api/generate/chat', {
                    method: 'POST',
                    body: formData
                });

                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }

                const result = await response.json();
                
                // Handle the response
                if (result.success) {
                    const userText = result.transcription || result.userInput || 'Audio processed';
                    const aiResponse = result.response || result.text || 'No response generated';
                    
                    // Add to conversation history
                    conversationHistory.push(
                        { role: 'user', content: userText },
                        { role: 'assistant', content: aiResponse }
                    );
                    
                    // Add to transcript
                    addToTranscript('You', userText);
                    addToTranscript('Assistant', aiResponse);
                    
                    // Speak the AI response using selected voice engine
                    await synthesizeVoice(aiResponse);
                    
                    updateStatus('‚úÖ Conversation complete');
                } else {
                    throw new Error(result.error || 'Chat request failed');
                }

            } catch (error) {
                console.error('Failed to process audio:', error);
                updateStatus('‚ùå Failed to process speech - try again');
                
                // Fallback to transcript message
                addToTranscript('System', 'Failed to process audio. Please try again.');
            }
        }

        // Real-time conversation is now handled by sendAudioToChat()
        // No need for simulated responses - using actual AI!

        // Transcript functionality
        function toggleTranscript() {
            const panel = document.getElementById('transcript-panel');
            panel.classList.toggle('hidden');
        }

        function addToTranscript(speaker, text) {
            const content = document.getElementById('transcript-content');
            const entry = document.createElement('div');
            entry.innerHTML = `<strong>${speaker}:</strong> ${text}`;
            entry.style.marginBottom = '10px';
            content.appendChild(entry);
            content.scrollTop = content.scrollHeight;
        }

        // Stop all audio and recording
        function stopAll() {
            // Stop any currently playing audio
            if (currentAudio) {
                currentAudio.pause();
                currentAudio.currentTime = 0;
            }
            if ('speechSynthesis' in window) {
                speechSynthesis.cancel();
            }
            
            // Stop voice sample recording if active
            if (voiceConfig.chatterbox.recording.isRecording) {
                stopRecording();
            }
            
            // Stop STT recording if active
            if (isListening) {
                stopListening();
            }
            
            updateStatus('‚èπÔ∏è All audio stopped');
        }

        // Update status display
        function updateStatus(message) {
            document.getElementById('current-status').textContent = message;
            console.log('Status:', message);
        }

        // Add some CSS for dynamic effects
        const style = document.createElement('style');
        style.textContent = `
            .mic-btn.listening {
                background: #f44336 !important;
                animation: pulse 1s infinite;
            }
            .transcript-panel.hidden {
                display: none;
            }
            .transcript-panel {
                position: fixed;
                top: 50px;
                right: 20px;
                width: 300px;
                height: 400px;
                background: white;
                border: 2px solid #4CAF50;
                border-radius: 10px;
                padding: 20px;
                box-shadow: 0 4px 20px rgba(0,0,0,0.3);
                z-index: 1000;
                overflow-y: auto;
            }
        `;
        document.head.appendChild(style);

        // Initialize with a welcome message
        setTimeout(() => {
            if (voiceConfig.enabled) {
                const engine = voiceConfig.selectedEngine;
                if (engine === 'chatterbox') {
                    synthesizeVoice("Hello! I'm your AI voice assistant with dual voice engines. Currently using Chatterbox TTS for high-quality neural synthesis with emotion control and voice cloning. Click the microphone to start a conversation, or switch to Edge TTS for faster responses!");
                } else {
                    synthesizeVoice("Hello! I'm your AI voice assistant using Edge TTS for fast, real-time conversation. I have hundreds of neural voices available in multiple languages. Click the microphone to start chatting, or switch to Chatterbox for advanced features like voice cloning!");
                }
            }
        }, 2000);
    </script>
</body>
</html> 