services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.8
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"      
      - xpack.security.enabled=false
      # Reduce log verbosity - only show warnings and errors
      - logger.level=WARN
      - logger.org.elasticsearch=WARN
      - logger.root=WARN
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      # Bind mount to host filesystem instead of Docker-managed volume
      # This prevents hitting Docker's disk image space limits
      # Set ELASTICSEARCH_DATA_PATH in .env to customize location (default: ./elasticsearch_data)
      - ${ELASTICSEARCH_DATA_PATH:-./elasticsearch_data}:/usr/share/elasticsearch/data
    ports:
      - "${ELASTICSEARCH_PORT:-9200}:9200"
      - "${ELASTICSEARCH_TRANSPORT_PORT:-9300}:9300"
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic
      - standard
      - standard-macMseries
      - minimal
      - minimal-with-scrape
      - gpu
      - standard-gpu

  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.8
    depends_on:
      - elasticsearch
    ports:
      - "${KIBANA_PORT:-5601}:5601"
    environment:
      - ELASTICCLIENTHOST=http://elasticsearch:9200
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic
      - standard
      - standard-macMseries
      - minimal
      - minimal-with-scrape
      - gpu
      - standard-gpu

  ipfs:
    image: ipfs/go-ipfs:latest
    environment:
      - IPFS_PROFILE=server
    ports:
      - "${IPFS_SWARM_PORT:-4001}:4001"
      - "${IPFS_API_PORT:-5001}:5001"
      - "${IPFS_GATEWAY_PORT:-8080}:8080"
    volumes:
      - ipfsdata:/data/ipfs
    networks:
      - oip-network
    profiles:
      - standard-monolithic
      - standard
      - standard-macMseries
      - standard-gpu

  # Standard OIP service (for distributed deployments with AI features)
  oip:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "${PORT:-3005}:${PORT:-3005}"
      - "${DEBUG_PORT:-9229}:9229"
    volumes:
      - ./data:/usr/src/app/data  # Media files and other data
      - ./helpers:/usr/src/app/helpers
      - ./routes:/usr/src/app/routes
      - ./voices:/usr/src/app/voices
      - ../public:/usr/src/parent-public:ro  # Mount parent directory's public folder
    environment:
      - NODE_ENV=production
      - NODE_OPTIONS=${NODE_OPTIONS}
      - ELASTICSEARCH_HOST=${ELASTICSEARCHHOST}
      - ELASTICCLIENTUSERNAME=${ELASTICCLIENTUSERNAME}
      - ELASTICCLIENTPASSWORD=${ELASTICCLIENTPASSWORD}
      - STT_SERVICE_URL=http://stt-service:8003
      - TTS_SERVICE_URL=http://tts-service:8005
    depends_on:
      - elasticsearch
      - tts-service
      - stt-service
      - ollama
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard
      - standard-macMseries

  # Minimal OIP service (core functionality only, no AI dependencies, no canvas)
  oip-minimal:
    build:
      context: .
      dockerfile: Dockerfile-minimal
    env_file:
      - .env
    ports:
      - "${PORT:-3005}:${PORT:-3005}"
      - "${DEBUG_PORT:-9229}:9229"
    volumes:
      - ./data:/usr/src/app/data  # Media files and other data
      - ./helpers:/usr/src/app/helpers
      - ./routes:/usr/src/app/routes
      - ./voices:/usr/src/app/voices
      - ../public:/usr/src/parent-public:ro  # Mount parent directory's public folder
    environment:
      - NODE_ENV=production
      - NODE_OPTIONS=${NODE_OPTIONS}
      - ELASTICSEARCH_HOST=${ELASTICSEARCHHOST}
      - ELASTICCLIENTUSERNAME=${ELASTICCLIENTUSERNAME}
      - ELASTICCLIENTPASSWORD=${ELASTICCLIENTPASSWORD}
    depends_on:
      - elasticsearch
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - minimal

  # Minimal OIP service with scraping capabilities (includes canvas)
  oip-minimal-with-scrape:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "${PORT:-3005}:${PORT:-3005}"
      - "${DEBUG_PORT:-9229}:9229"
    volumes:
      - ./data:/usr/src/app/data  # Media files and other data
      - ./helpers:/usr/src/app/helpers
      - ./routes:/usr/src/app/routes
      - ./voices:/usr/src/app/voices
      - ../public:/usr/src/parent-public:ro  # Mount parent directory's public folder
    environment:
      - NODE_ENV=production
      - NODE_OPTIONS=${NODE_OPTIONS}
      - ELASTICSEARCH_HOST=${ELASTICSEARCHHOST}
      - ELASTICCLIENTUSERNAME=${ELASTICCLIENTUSERNAME}
      - ELASTICCLIENTPASSWORD=${ELASTICCLIENTPASSWORD}
    depends_on:
      - elasticsearch
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - minimal-with-scrape

  # Standard monolithic OIP service (all services in one container)
  oip-full:
    build:
      context: .
      dockerfile: Dockerfile-full
    env_file:
      - .env
    ports:
      - "${PORT:-3005}:${PORT:-3005}"
      - "${DEBUG_PORT:-9229}:9229"
      - "${TEXT_GENERATOR_PORT:-8081}:8081"  # Text generator
      - "${SPEECH_SYNTHESIZER_PORT:-8082}:8082"  # Speech synthesizer
      - "${NGROK_DASHBOARD_PORT:-4040}:4040"  # Ngrok dashboard
    volumes:
      - ./data:/usr/src/app/data  # Media files and other data
      - ./helpers:/usr/src/app/helpers
      - ./routes:/usr/src/app/routes
      - ../public:/usr/src/parent-public:ro  # Mount parent directory's public folder
    environment:
      - NODE_ENV=production
      - NODE_OPTIONS=${NODE_OPTIONS}
      - ELASTICSEARCH_HOST=${ELASTICSEARCHHOST}
      - ELASTICCLIENTUSERNAME=${ELASTICCLIENTUSERNAME}
      - ELASTICCLIENTPASSWORD=${ELASTICCLIENTPASSWORD}
      - NGROK_AUTHTOKEN=${NGROK_AUTH_TOKEN}
    depends_on:
      - elasticsearch
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic

  # GPU-optimized OIP service
  oip-gpu:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "${NEXT_FRONTEND_PORT:-3000}:3000"  # Next.js frontend
      - "${PORT:-3005}:${PORT:-3005}"  # Express API
      - "${DEBUG_PORT:-9229}:9229"
    volumes:
      - ./data:/usr/src/app/data  # Media files and other data
      - ./helpers:/usr/src/app/helpers
      - ./routes:/usr/src/app/routes
      - ./voices:/usr/src/app/voices
      - ../public:/usr/src/parent-public:ro  # Mount parent directory's public folder
    environment:
      - NODE_ENV=production
      - NODE_OPTIONS=${NODE_OPTIONS}
      - ELASTICSEARCH_HOST=${ELASTICSEARCHHOST}
      - ELASTICCLIENTUSERNAME=${ELASTICCLIENTUSERNAME}
      - ELASTICCLIENTPASSWORD=${ELASTICCLIENTPASSWORD}
      - STT_SERVICE_URL=http://stt-service-gpu:8003
      - TTS_SERVICE_URL=http://tts-service-gpu:5002
      - OLLAMA_HOST=http://ollama-gpu:11434  # Internal Docker network port stays 11434
      - DEFAULT_LLM_MODEL=llama3.2:3b
    depends_on:
      - elasticsearch
      - ollama-gpu
      - tts-service-gpu
      - stt-service-gpu
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - gpu
      - oip-gpu-only
      - standard-gpu

  # Self-hosted Speech Synthesizer (Coqui TTS) - for distributed deployments
  speech-synthesizer:
    build:
      context: ./speech-synthesizer
      dockerfile: Dockerfile
    ports:
      - "${SPEECH_SYNTHESIZER_PORT:-8082}:8082"
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard
      - standard-macMseries

  # GPU-accelerated TTS Service (Neural Models with CUDA)
  tts-service-gpu:
    build:
      context: ./text-to-speech
      dockerfile: Dockerfile.gpu
    ports:
      - "${TTS_SERVICE_PORT:-5002}:5002"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - TTS_GPU_ENABLED=true
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - gpu
      - standard-gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Ollama LLM service for standard deployments (CPU/Apple Silicon)
  ollama:
    image: ollama/ollama:latest
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ./ollama_data:/root/.ollama  # Persistent model storage
    networks:
      - oip-network
    restart: unless-stopped
    dns:
      - 8.8.8.8
      - 8.8.4.4
    profiles:
      - standard
      - standard-macMseries
      - standard-monolithic

  # Ollama LLM service for GPU deployments (NVIDIA CUDA)
  ollama-gpu:
    image: ollama/ollama:latest
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ./ollama_data:/root/.ollama  # Persistent model storage
    networks:
      - oip-network
    restart: unless-stopped
    dns:
      - 8.8.8.8
      - 8.8.4.4
    profiles:
      - standard-gpu
      - gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Text generator for standard deployments (CPU/Apple Silicon)
  text-generator:
    build: ./text-generator
    env_file:
      - .env
    environment:
      - OLLAMA_HOST=http://ollama:11434  # Internal Docker network port stays 11434
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    ports:
      - "${TEXT_GENERATOR_PORT:-8081}:8081"
    depends_on:
      - ollama
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard
      - standard-macMseries
      - standard-monolithic

  # Speech-to-Text Service (Whisper)
  stt-service:
    build: ./speech-to-text
    environment:
      - WHISPER_MODEL=${WHISPER_MODEL:-base}
      - WHISPER_DEVICE=${WHISPER_DEVICE:-cpu}
      - WHISPER_COMPUTE_TYPE=${WHISPER_COMPUTE_TYPE:-int8}
    ports:
      - "${STT_SERVICE_PORT:-8003}:8003"
    volumes:
      - whisper_models:/app/models
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic
      - standard
      - standard-macMseries

  # GPU-accelerated Speech-to-Text Service (Whisper with CUDA)
  stt-service-gpu:
    build:
      context: ./speech-to-text
      dockerfile: Dockerfile.gpu
    environment:
      - WHISPER_MODEL=${WHISPER_MODEL:-base}
      - WHISPER_DEVICE=cpu
      - WHISPER_COMPUTE_TYPE=int8
      - CUDA_VISIBLE_DEVICES=0
    ports:
      - "${STT_SERVICE_PORT:-8003}:8003"
    volumes:
      - whisper_models:/app/models
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-gpu
      - gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Text-to-Speech Service (Multi-engine)
  tts-service:
    build: ./text-to-speech
    ports:
      - "${TTS_SERVICE_PORT:-8005}:8005"  # Standard port for TTS service
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic
      - standard
      - standard-macMseries

  # Ngrok for public access - DISABLED: Using Makefile ngrok with custom domain instead
  # ngrok:
  #   image: ngrok/ngrok
  #   command: ["ngrok", "http", "oip:3005", "--log=stdout"]
  #   environment:
  #     - NGROK_AUTHTOKEN=${NGROK_AUTH_TOKEN}
  #   depends_on:
  #     - oip
  #   ports:
  #     - "4040:4040"
  #   networks:
  #     - oip-network
  #   restart: unless-stopped
  #   profiles:
  #     - standard
  
  # Ngrok for public access
  ngrok:
    image: ngrok/ngrok
    command: http --domain=${NGROK_DOMAIN} oip:${PORT:-3005}
    environment:
      - NGROK_AUTHTOKEN=${NGROK_AUTH_TOKEN}
    depends_on:
      - oip
    ports:
      - "${NGROK_DASHBOARD_PORT:-4040}:4040"
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard
      - standard-macMseries

  # Ngrok for minimal profile
  ngrok-minimal:
    image: ngrok/ngrok
    command: http --domain=${NGROK_DOMAIN} oip-minimal:${PORT:-3005}
    environment:
      - NGROK_AUTHTOKEN=${NGROK_AUTH_TOKEN}
    depends_on:
      - oip-minimal
    ports:
      - "${NGROK_DASHBOARD_PORT:-4040}:4040"
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - minimal

  # Ngrok for GPU deployments - Uses configurable domain from .env
  ngrok-gpu:
    image: ngrok/ngrok:latest
    command: http --domain=${NGROK_DOMAIN} oip-gpu:${PORT:-3005}
    environment:
      - NGROK_AUTHTOKEN=${NGROK_AUTH_TOKEN}
    depends_on:
      - oip-gpu
    ports:
      - "${NGROK_DASHBOARD_PORT:-4040}:4040"  # Use configurable port to avoid conflicts
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-gpu

  # GUN Relay Service for private/temporary storage with multi-node sync support
  gun-relay:
    image: node:18-alpine
    working_dir: /app
    command: sh -c "npm init -y >/dev/null 2>&1 && npm i gun && node gun-relay-server.js"
    ports:
      - "${GUN_RELAY_PORT:-8765}:8765"
    volumes:
      - gundata:/app/data
      - ./gun-relay-server.js:/app/gun-relay-server.js:ro
    environment:
      # Configure GUN peers for multi-node synchronization
      - GUN_PEERS=${GUN_EXTERNAL_PEERS:-}
      - GUN_ENV=production
      - GUN_SYNC_ENABLED=true
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - minimal
      - minimal-with-scrape
      - standard
      - standard-macMseries
      - standard-gpu
      - standard-monolithic

networks:
  oip-network:
    name: ${COMPOSE_PROJECT_NAME:-oip-arweave-indexer}_oip-network
    driver: bridge

volumes:
  # Note: esdata removed - now using bind mount to host filesystem (see elasticsearch service)
  # This prevents hitting Docker's disk image space limits
  ipfsdata:
    name: ${COMPOSE_PROJECT_NAME:-oip-arweave-indexer}_ipfsdata
  whisper_models:
    name: ${COMPOSE_PROJECT_NAME:-oip-arweave-indexer}_whisper_models
  gundata:
    name: ${COMPOSE_PROJECT_NAME:-oip-arweave-indexer}_gundata
