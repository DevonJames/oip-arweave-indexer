services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.8
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"      
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic
      - standard
      - minimal
      - minimal-with-scrape
      - gpu
      - standard-gpu

  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.8
    depends_on:
      - elasticsearch
    ports:
      - "5601:5601"
    environment:
      - ELASTICCLIENTHOST=http://elasticsearch:9200
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic
      - standard
      - minimal
      - minimal-with-scrape
      - gpu
      - standard-gpu

  ipfs:
    image: ipfs/go-ipfs:latest
    environment:
      - IPFS_PROFILE=server
    ports:
      - "4001:4001"
      - "5001:5001"
      - "8080:8080"
    volumes:
      - ipfsdata:/data/ipfs
    networks:
      - oip-network
    profiles:
      - standard-monolithic
      - standard
      - standard-gpu

  # Standard OIP service (for distributed deployments with AI features)
  oip:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "3005:3005"
      - "9229:9229"
    volumes:
      - ./media:/usr/src/app/media
      - ./helpers:/usr/src/app/helpers
      - ./routes:/usr/src/app/routes
      - ./voices:/usr/src/app/voices
    environment:
      - NODE_ENV=production
      - ELASTICSEARCH_HOST=${ELASTICSEARCHHOST}
      - ELASTICCLIENTUSERNAME=${ELASTICCLIENTUSERNAME}
      - ELASTICCLIENTPASSWORD=${ELASTICCLIENTPASSWORD}
      - STT_SERVICE_URL=http://stt-service:8003
      - TTS_SERVICE_URL=http://tts-service:8005
      - TEXT_GENERATOR_URL=http://text-generator:8081
    depends_on:
      - elasticsearch
      - text-generator
      - tts-service
      - stt-service
      - ollama
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard

  # Minimal OIP service (core functionality only, no AI dependencies, no canvas)
  oip-minimal:
    build:
      context: .
      dockerfile: Dockerfile-minimal
    env_file:
      - .env
    ports:
      - "3005:3005"
      - "9229:9229"
    volumes:
      - ./media:/usr/src/app/media
      - ./helpers:/usr/src/app/helpers
      - ./routes:/usr/src/app/routes
      - ./voices:/usr/src/app/voices
    environment:
      - NODE_ENV=production
      - ELASTICSEARCH_HOST=${ELASTICSEARCHHOST}
      - ELASTICCLIENTUSERNAME=${ELASTICCLIENTUSERNAME}
      - ELASTICCLIENTPASSWORD=${ELASTICCLIENTPASSWORD}
    depends_on:
      - elasticsearch
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - minimal

  # Minimal OIP service with scraping capabilities (includes canvas)
  oip-minimal-with-scrape:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "3005:3005"
      - "9229:9229"
    volumes:
      - ./media:/usr/src/app/media
      - ./helpers:/usr/src/app/helpers
      - ./routes:/usr/src/app/routes
      - ./voices:/usr/src/app/voices
    environment:
      - NODE_ENV=production
      - ELASTICSEARCH_HOST=${ELASTICSEARCHHOST}
      - ELASTICCLIENTUSERNAME=${ELASTICCLIENTUSERNAME}
      - ELASTICCLIENTPASSWORD=${ELASTICCLIENTPASSWORD}
    depends_on:
      - elasticsearch
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - minimal-with-scrape

  # Standard monolithic OIP service (all services in one container)
  oip-full:
    build:
      context: .
      dockerfile: Dockerfile-full
    env_file:
      - .env
    ports:
      - "3005:3005"
      - "9229:9229"
      - "8081:8081"  # Text generator
      - "8082:8082"  # Speech synthesizer
      - "4040:4040"  # Ngrok dashboard
    volumes:
      - ./media:/usr/src/app/media
      - ./helpers:/usr/src/app/helpers
      - ./routes:/usr/src/app/routes
    environment:
      - NODE_ENV=production
      - ELASTICSEARCH_HOST=${ELASTICSEARCHHOST}
      - ELASTICCLIENTUSERNAME=${ELASTICCLIENTUSERNAME}
      - ELASTICCLIENTPASSWORD=${ELASTICCLIENTPASSWORD}
      - NGROK_AUTHTOKEN=${NGROK_AUTH_TOKEN}
    depends_on:
      - elasticsearch
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic

  # GPU-optimized OIP service
  oip-gpu:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "3000:3000"  # Next.js frontend
      - "3005:3005"  # Express API
      - "9229:9229"
    volumes:
      - ./media:/usr/src/app/media
      - ./helpers:/usr/src/app/helpers
      - ./routes:/usr/src/app/routes
      - ./voices:/usr/src/app/voices
    environment:
      - NODE_ENV=production
      - ELASTICSEARCH_HOST=${ELASTICSEARCHHOST}
      - ELASTICCLIENTUSERNAME=${ELASTICCLIENTUSERNAME}
      - ELASTICCLIENTPASSWORD=${ELASTICCLIENTPASSWORD}
      - STT_SERVICE_URL=http://stt-service-gpu:8003
      - TTS_SERVICE_URL=http://tts-service-gpu:5002
      - OLLAMA_HOST=http://ollama-gpu:11434
      - DEFAULT_LLM_MODEL=llama3.2:3b
    depends_on:
      - elasticsearch
      - ollama-gpu
      - tts-service-gpu
      - stt-service-gpu
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - gpu
      - oip-gpu-only
      - standard-gpu

  # Self-hosted Speech Synthesizer (Coqui TTS) - for distributed deployments
  speech-synthesizer:
    build:
      context: ./speech-synthesizer
      dockerfile: Dockerfile
    ports:
      - "8082:8082"
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard

  # GPU-accelerated TTS Service (Neural Models with CUDA)
  tts-service-gpu:
    build:
      context: ./text-to-speech
      dockerfile: Dockerfile.gpu
    ports:
      - "5002:5002"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - TTS_GPU_ENABLED=true
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - gpu
      - standard-gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Ollama LLM service for standard deployments (CPU/Apple Silicon)
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama  # Persistent model storage
    networks:
      - oip-network
    restart: unless-stopped
    dns:
      - 8.8.8.8
      - 8.8.4.4
    profiles:
      - standard
      - standard-monolithic

  # Ollama LLM service for GPU deployments (NVIDIA CUDA)
  ollama-gpu:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama  # Persistent model storage
    networks:
      - oip-network
    restart: unless-stopped
    dns:
      - 8.8.8.8
      - 8.8.4.4
    profiles:
      - standard-gpu
      - gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Text generator for standard deployments (CPU/Apple Silicon)
  text-generator:
    build: ./text-generator
    env_file:
      - .env
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    ports:
      - "8081:8081"
    depends_on:
      - ollama
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard
      - standard-monolithic

  # Text generator for GPU deployments - uses GPU Ollama API
  text-generator-gpu:
    build: ./text-generator
    env_file:
      - .env
    environment:
      - OLLAMA_HOST=http://ollama-gpu:11434
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    ports:
      - "8081:8081"
    depends_on:
      - ollama-gpu
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-gpu
      - gpu

  # Speech-to-Text Service (Whisper)
  stt-service:
    build: ./speech-to-text
    environment:
      - WHISPER_MODEL=${WHISPER_MODEL:-base}
      - WHISPER_DEVICE=${WHISPER_DEVICE:-cpu}
      - WHISPER_COMPUTE_TYPE=${WHISPER_COMPUTE_TYPE:-int8}
    ports:
      - "8003:8003"
    volumes:
      - whisper_models:/app/models
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic
      - standard

  # GPU-accelerated Speech-to-Text Service (Whisper with CUDA)
  stt-service-gpu:
    build:
      context: ./speech-to-text
      dockerfile: Dockerfile.gpu
    environment:
      - WHISPER_MODEL=${WHISPER_MODEL:-base}
      - WHISPER_DEVICE=cpu
      - WHISPER_COMPUTE_TYPE=int8
      - CUDA_VISIBLE_DEVICES=0
    ports:
      - "8003:8003"
    volumes:
      - whisper_models:/app/models
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-gpu
      - gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Text-to-Speech Service (Multi-engine)
  tts-service:
    build: ./text-to-speech
    ports:
      - "8006:8005"  # Different port to avoid conflict with GPU service
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic
      - standard

  # Ngrok for public access - DISABLED: Using Makefile ngrok with custom domain instead
  # ngrok:
  #   image: ngrok/ngrok
  #   command: ["ngrok", "http", "oip:3005", "--log=stdout"]
  #   environment:
  #     - NGROK_AUTHTOKEN=${NGROK_AUTH_TOKEN}
  #   depends_on:
  #     - oip
  #   ports:
  #     - "4040:4040"
  #   networks:
  #     - oip-network
  #   restart: unless-stopped
  #   profiles:
  #     - standard
  
  # Ngrok for public access
  ngrok:
    image: ngrok/ngrok
    command: http --domain=${NGROK_DOMAIN} oip:3005
    environment:
      - NGROK_AUTHTOKEN=${NGROK_AUTH_TOKEN}
    depends_on:
      - oip
    ports:
      - "4040:4040"
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard

  # Ngrok for minimal profile
  ngrok-minimal:
    image: ngrok/ngrok
    command: http --domain=${NGROK_DOMAIN} oip-minimal:3005
    environment:
      - NGROK_AUTHTOKEN=${NGROK_AUTH_TOKEN}
    depends_on:
      - oip-minimal
    ports:
      - "4040:4040"
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - minimal

  # Ngrok for GPU deployments - Fixed for RTX 4090 server deployments
  ngrok-gpu:
    image: ngrok/ngrok:latest
    command: http --domain=api.oip.onl oip-gpu:3005
    environment:
      - NGROK_AUTHTOKEN=${NGROK_AUTH_TOKEN}
    depends_on:
      - oip-gpu
    ports:
      - "4042:4040"  # Use port 4042 to avoid conflicts with other ngrok instances
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-gpu

  # GUN Relay Service for private/temporary storage
  gun-relay:
    image: node:18-alpine
    working_dir: /app
    command: >
      sh -c "npm init -y >/dev/null 2>&1 || true && 
      npm i gun && 
      node -e 'const Gun = require(\"gun\"); require(\"gun/sea\"); const http = require(\"http\"); const server = http.createServer(); Gun({ web: server, radisk: true, file: \"data\" }); server.listen(8765, () => console.log(\"GUN relay running on :8765\"));'"
    ports:
      - "8765:8765"
    volumes:
      - gundata:/app/data
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard
      - standard-gpu
      - standard-monolithic

networks:
  oip-network:
    driver: bridge

volumes:
  esdata:
  ipfsdata:
  whisper_models:
  gundata:
