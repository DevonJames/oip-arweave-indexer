services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.8
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"      
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic
      - standard
      - minimal
      - gpu
      - standard-gpu

  kibana:
    image: docker.elastic.co/kibana/kibana:7.17.8
    depends_on:
      - elasticsearch
    ports:
      - "5601:5601"
    environment:
      - ELASTICCLIENTHOST=http://elasticsearch:9200
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic
      - standard
      - minimal
      - gpu
      - standard-gpu

  ipfs:
    image: ipfs/go-ipfs:latest
    environment:
      - IPFS_PROFILE=server
    ports:
      - "4001:4001"
      - "5001:5001"
      - "8080:8080"
    volumes:
      - ipfsdata:/data/ipfs
    networks:
      - oip-network
    profiles:
      - standard-monolithic
      - standard
      - standard-gpu

  # Standard OIP service (for distributed deployments)
  oip:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "3005:3005"
      - "9229:9229"
    volumes:
      - ./helpers:/usr/src/app/helpers
      - ./routes:/usr/src/app/routes
    environment:
      - NODE_ENV=production
      - ELASTICSEARCH_HOST=${ELASTICSEARCHHOST}
      - ELASTICCLIENTUSERNAME=${ELASTICCLIENTUSERNAME}
      - ELASTICCLIENTPASSWORD=${ELASTICCLIENTPASSWORD}
    depends_on:
      - elasticsearch
      - text-generator
      - ollama
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard
      - minimal

  # Standard monolithic OIP service (all services in one container)
  oip-full:
    build:
      context: .
      dockerfile: Dockerfile-full
    env_file:
      - .env
    ports:
      - "3005:3005"
      - "9229:9229"
      - "8081:8081"  # Text generator
      - "8082:8082"  # Speech synthesizer
      - "4040:4040"  # Ngrok dashboard
    volumes:
      - ./helpers:/usr/src/app/helpers
      - ./routes:/usr/src/app/routes
    environment:
      - NODE_ENV=production
      - ELASTICSEARCH_HOST=${ELASTICSEARCHHOST}
      - ELASTICCLIENTUSERNAME=${ELASTICCLIENTUSERNAME}
      - ELASTICCLIENTPASSWORD=${ELASTICCLIENTPASSWORD}
      - NGROK_AUTHTOKEN=${NGROK_AUTH_TOKEN}
    depends_on:
      - elasticsearch
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic

  # GPU-optimized OIP service
  oip-gpu:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env
    ports:
      - "3000:3000"  # Next.js frontend
      - "3005:3005"  # Express API
      - "9229:9229"
    volumes:
      - ./media:/usr/src/app/media
      - ./helpers:/usr/src/app/helpers
      - ./routes:/usr/src/app/routes
    environment:
      - NODE_ENV=production
      - ELASTICSEARCH_HOST=${ELASTICSEARCHHOST}
      - ELASTICCLIENTUSERNAME=${ELASTICCLIENTUSERNAME}
      - ELASTICCLIENTPASSWORD=${ELASTICCLIENTPASSWORD}
      - STT_SERVICE_URL=http://stt-service-gpu:8003
      - TTS_SERVICE_URL=http://tts-service-gpu:5002
      - TEXT_GENERATOR_URL=http://text-generator:8081
    depends_on:
      - elasticsearch
      - text-generator-gpu
      - tts-service-gpu
      - stt-service-gpu
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - gpu
      - oip-gpu-only
      - standard-gpu

  # Self-hosted Speech Synthesizer (Coqui TTS) - for distributed deployments
  speech-synthesizer:
    build:
      context: ./speech-synthesizer
      dockerfile: Dockerfile
    ports:
      - "8082:8082"
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard

  # GPU-accelerated TTS Service (Neural Models with CUDA)
  tts-service-gpu:
    build:
      context: ./text-to-speech
      dockerfile: Dockerfile.gpu
    ports:
      - "5002:5002"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - TTS_GPU_ENABLED=true
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - gpu
      - standard-gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Ollama LLM service for standard deployments (CPU/Apple Silicon)
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama  # Persistent model storage
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard
      - standard-monolithic

  # Ollama LLM service for GPU deployments (NVIDIA CUDA)
  ollama-gpu:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama  # Persistent model storage
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-gpu
      - gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Text generator for standard deployments (CPU/Apple Silicon)
  text-generator:
    build: ./text-generator
    env_file:
      - .env
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    ports:
      - "8081:8081"
    depends_on:
      - ollama
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard
      - standard-monolithic

  # Text generator for GPU deployments - uses GPU Ollama API
  text-generator-gpu:
    build: ./text-generator
    env_file:
      - .env
    environment:
      - OLLAMA_HOST=http://ollama-gpu:11434
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
    ports:
      - "8081:8081"
    depends_on:
      - ollama-gpu
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-gpu
      - gpu

  # Speech-to-Text Service (Whisper)
  stt-service:
    build: ./speech-to-text
    environment:
      - WHISPER_MODEL=${WHISPER_MODEL:-base}
      - WHISPER_DEVICE=${WHISPER_DEVICE:-cpu}
      - WHISPER_COMPUTE_TYPE=${WHISPER_COMPUTE_TYPE:-int8}
    ports:
      - "8003:8003"
    volumes:
      - whisper_models:/app/models
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic
      - standard

  # GPU-accelerated Speech-to-Text Service (Whisper with CUDA)
  stt-service-gpu:
    build:
      context: ./speech-to-text
      dockerfile: Dockerfile.gpu
    environment:
      - WHISPER_MODEL=${WHISPER_MODEL:-base}
      - WHISPER_DEVICE=cpu
      - WHISPER_COMPUTE_TYPE=int8
      - CUDA_VISIBLE_DEVICES=0
    ports:
      - "8003:8003"
    volumes:
      - whisper_models:/app/models
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-gpu
      - gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Text-to-Speech Service (Multi-engine)
  tts-service:
    build: ./text-to-speech
    ports:
      - "8006:8005"  # Different port to avoid conflict with GPU service
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-monolithic
      - standard

  # Ngrok for public access - for distributed deployments
  ngrok:
    image: ngrok/ngrok
    command: ["ngrok", "http", "oip:3005", "--log=stdout"]
    environment:
      - NGROK_AUTHTOKEN=${NGROK_AUTH_TOKEN}
    depends_on:
      - oip
    ports:
      - "4040:4040"
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard

  # Ngrok for GPU deployments
  ngrok-gpu:
    image: ngrok/ngrok:latest
    command: ["ngrok", "http", "oip-gpu:3000", "--log=stdout"]
    environment:
      - NGROK_AUTHTOKEN=${NGROK_AUTH_TOKEN}
    depends_on:
      - oip-gpu
    ports:
      - "4040:4040"
    networks:
      - oip-network
    restart: unless-stopped
    profiles:
      - standard-gpu

networks:
  oip-network:
    driver: bridge
    name: oiparweave_oip-network

volumes:
  esdata:
  ipfsdata:
  whisper_models:
