# Memory Leak Investigation - Ruled Out Theories

## Summary
This document tracks all theories that have been investigated and **ruled out** as primary causes of the memory leak in the OIP Arweave Indexer. This prevents starting from square one in future investigations.

---

## ‚úÖ Ruled Out (Fixed or Not Primary Cause)

### 1. Voice/TTS Audio Buffer Leak
**Theory**: `fs.readFileSync()` creating new buffers per request, with buffers held in `ongoingDialogues` Map indefinitely.

**Investigation**:
- Found: `fs.readFileSync()` for thinking sounds, `toString('base64')` increasing size
- Found: Buffers retained in `ongoingDialogues` Map
- **Fix Applied**: Eliminated thinking sound transfer, aggressive dialogue cleanup on disconnect

**Status**: ‚úÖ Fixed - not the primary leak source, but was contributing ~50-100MB per active voice session

**Evidence**: After fix, leak continued at same rate with no voice activity

---

### 2. Elasticsearch/Undici Buffer Accumulation
**Theory**: Undici HTTP client (used by Elasticsearch client) accumulating response buffers not released until client closed.

**Investigation**:
- Found: Elasticsearch client creating persistent Undici connections
- Found: Buffers accumulating in Undici's internal pools
- **Fix Applied**: Periodic Elasticsearch client recreation every 5 minutes with forced GC

**Status**: ‚úÖ Fixed - was contributing, but not the primary leak

**Evidence**: After fix, leak rate reduced from ~500 MB/min to ~400 MB/min, but leak continued

---

### 3. GraphQL Client Socket Leak
**Theory**: `graphql-request` creating unmanaged HTTP connections leading to Socket and TCPConnectWrap handle accumulation.

**Investigation**:
- Found: `graphql-request.request()` creating new connections per request
- Found: Sockets not being closed properly
- **Fix Applied**: Replaced with `GraphQLClient` using `node-fetch` and explicit HTTP agents with `keepAlive: false`, periodic client recreation every 30 minutes

**Status**: ‚úÖ Fixed - was contributing to handle count, but not external memory leak

**Evidence**: Socket count stabilized, but external memory continued growing

---

### 4. Static GIF Serving Leak
**Theory**: `express.static()` buffering large GIF files in memory without proper caching.

**Investigation**:
- Found: Numerous requests for `/api/media/fitnessally/*.gif` files (50-500KB each)
- Found: No browser caching configured
- **Fix Applied**: Aggressive browser caching (`maxAge: '1y'`, `immutable: true`) and forced GC after static responses

**Status**: ‚úÖ Fixed - was contributing ~10-20 MB/min, but not primary leak

**Evidence**: GIF requests reduced significantly due to caching, but leak continued

---

### 5. Memory Leak Tracker Self-Interference
**Theory**: The memory leak tracker itself, by calling `process._getActiveHandles()` and `process._getActiveRequests()` and iterating over leaked objects, was holding references and interfering with GC.

**Investigation**:
- Found: Tracker calling `_getActiveHandles()` and `_getActiveRequests()` every 3 minutes
- Found: During severe leaks, this could iterate over thousands of objects
- **Fix Applied**: Circuit breaker to disable handle/request tracking when external memory >50GB

**Status**: ‚úÖ Mitigated - was potentially exacerbating severe leaks, but not causing them

**Evidence**: Circuit breaker prevented tracker interference during critical memory conditions

---

### 6. GUN Deletion Registry Bugs (Infinite Loop)
**Theory**: Three bugs causing an infinite deletion loop: uninitialized `this.gun` in `deleteRecord()`, invalid Elasticsearch `ignore` parameter, and lack of reprocessing prevention.

**Investigation**:
- Found: `Cannot read properties of undefined (reading 'get')` errors during deletion
- Found: `illegal_argument_exception: unrecognized parameter: [ignore]` in ES delete calls
- Found: Same 5 workout records being deleted repeatedly in every sync cycle
- **Fix Applied**: 
  1. Refactored `GunHelper.deleteRecord()` to use HTTP API
  2. Removed invalid `ignore` parameter, used try-catch for 404s
  3. Added `recentlyProcessed` Map to prevent reprocessing within 24 hours

**Status**: ‚úÖ Fixed - eliminated the infinite deletion loop

**Evidence**: Deletion errors stopped, but memory leak continued at similar rate (~400 MB/min)

---

### 7. Axios Response Buffer Retention
**Theory**: Axios response buffers (especially JSON responses) not being cleaned up after use.

**Investigation**:
- Found: Large JSON responses being kept in memory
- **Fix Applied**: Aggressive buffer cleanup with `response.data = null` after 2-second delay, forced GC on large responses

**Status**: ‚úÖ Mitigated - reduced some memory accumulation, but not primary leak

**Evidence**: Some improvement in JSON response handling, but external memory still growing rapidly

---

### 8. GUN Sync Record Processing Errors (Infinite Retries)
**Theory**: Records failing with `mapper_parsing_exception` or `illegal_argument_exception` being retried infinitely, with Axios error buffers accumulating.

**Investigation**:
- Found: `oip` node logs showing `mapper_parsing_exception` for `conversationSession` records
- Found: `illegal_argument_exception` for `shoppingList` records
- Found: These errors leading to `500` responses from gun-relay
- Found: Correlation between error count and leak rate:
  - Rockhoppers: 0 errors ‚Üí 0 MB/min leak
  - OIP: 1 error ‚Üí ~30 MB/min leak
  - FitnessAlly: 2 errors ‚Üí ~500 MB/min leak
- **Fix Applied**: 
  1. Implemented `permanentlyFailedRecords` Set to skip records with persistent errors
  2. Aggressive Axios buffer cleanup in `gun.js` (`response.data = null`, `error.response = null`)
  3. Forced `global.gc()` after failed GUN operations

**Status**: ‚ùì **Fix applied but USER REPORTED STILL LEAKING** - user said "didnt work" and "still growing"

**Evidence**: Despite fix, user reported memory still reaching 10GB after ~3 hours with only 15 minutes of active use, with numerous `‚ö†Ô∏è Error in getRecord after 2 retries: Request failed with status code 404` messages and `EXTERNAL MEMORY LEAK DETECTED` warnings showing ~300-400 MB/min growth

---

### 9. Organization Decryption Queue Errors
**Theory**: `index_not_found_exception` when querying `organization_decrypt_queue` causing resource issues.

**Investigation**:
- Found: Error thrown on every login when queue index doesn't exist
- **Fix Applied**: Graceful error handling to log informational message instead of throwing

**Status**: ‚úÖ Fixed - was causing noise, but not a memory leak

**Evidence**: Error handling improved, no memory impact

---

### 10. Logging Verbosity
**Theory**: Excessive console logging contributing to Docker log buffer accumulation.

**Investigation**:
- Found: Massive amounts of logging:
  - `[ALFRED]` processing logs repeated for every query
  - Exercise instructions printed multiple times
  - GUN sync verbose operation logs
  - Memory tracker detailed output
- **Fix Applied**: Reduced logging verbosity across:
  - `helpers/alfred.js`
  - `helpers/gunDeletionRegistry.js`
  - `helpers/oipGunRegistry.js`
  - `helpers/gunSyncService.js`
  - `helpers/memoryTracker.js`
  - `routes/voice.js`
  - `helpers/elasticsearch.js`

**Status**: ‚úÖ **MAJOR CONTRIBUTOR DISCOVERED!** 

**Evidence**: User closed `docker logs -f` terminal and memory **dropped from 10GB to 5GB immediately** - **5GB reduction!**

**Conclusion**: The Docker logging stream was buffering massive amounts of log output. Even with reduced verbosity, the volume of logs (ALFRED processing, GUN sync, API requests) was causing Docker's stdout/stderr buffers to accumulate ~5GB of data.

---

## üîç Still Under Investigation

### 1. Remaining ~5GB After Closing Logs
**Current State**: After closing `docker logs -f`, memory dropped to 5GB and stabilized

**Questions**:
- Is this 5GB the "normal" operating memory?
- Is there still a slower leak happening?
- What's the memory composition of these 5GB?

**Next Steps**: 
- Enable the new `memoryDiagnostics` system
- Let it run for a few hours without `docker logs -f`
- Monitor if memory continues growing from 5GB or stabilizes

---

### 2. Potential Remaining Sources

#### A. Elasticsearch Response Buffers
- Even with client recreation, large query results might accumulate
- Need heap dump analysis to confirm

#### B. GUN Record Cache
- `processedRecords` Set and operation caches might grow over time
- Currently has cache clearing every hour, but might need more aggressive cleanup

#### C. Express/Node.js HTTP Buffers
- Request/response buffers for API calls
- Might need more aggressive cleanup on response end

#### D. ALFRED/LLM Context Accumulation
- Conversation history and RAG results being retained
- `ongoingDialogues` cleanup might not be aggressive enough

---

## üéØ Key Learnings

1. **Docker Logging is a Memory Leak Vector**: Running `docker logs -f` with high-volume logging can cause multi-GB memory accumulation in Docker's buffers. This was responsible for ~5GB (50%) of the leak.

2. **Multiple Small Leaks Add Up**: Many of the "fixed" issues were each contributing 10-100 MB/min, which adds up to significant growth over hours.

3. **Correlation ‚â† Causation**: High GUN sync error rates correlated with leak rates, but fixing the errors didn't eliminate the leak - the logging about the errors was a bigger factor.

4. **Observability Can Cause Problems**: Extensive logging and memory tracking can themselves cause memory issues, especially when logs are being consumed via `docker logs -f`.

5. **External vs. Heap Memory**: Most of the leak was external memory (ArrayBuffers, network buffers), not V8 heap, which made it harder to trace with traditional profiling.

---

## üìã Recommended Next Steps

1. **Test with No Log Tailing**: Run the app without `docker logs -f` for several hours and monitor memory via `docker stats` instead

2. **Enable New Diagnostics**: Use the new `memoryDiagnostics` system (which has minimal logging overhead) to identify remaining leak sources

3. **Reduce Logging Further**: Consider:
   - Setting a log level (ERROR only in production)
   - Using structured logging with sampling
   - Disabling verbose debug logs entirely

4. **Monitor Baseline**: Determine what "normal" memory usage should be at steady state (likely 2-3GB for a Node.js app of this size)

5. **Heap Dump Analysis**: If memory continues growing from 5GB, take heap dumps and analyze to find what objects are accumulating

---

## üö´ Things NOT to Try Again

1. ‚ùå Running `docker logs -f` during long-term monitoring
2. ‚ùå Adding more verbose logging for diagnostics
3. ‚ùå Repeatedly trying to fix GUN sync errors (they're schema mismatches that need app-level fixes)
4. ‚ùå Assuming the leak is in a single place (it's been multiple sources)
5. ‚ùå Using memory profiling tools that themselves log extensively

---

## ‚úÖ Successful Mitigations Applied

1. ‚úÖ Periodic Elasticsearch client recreation (every 5 min)
2. ‚úÖ Periodic GraphQL client recreation (every 30 min)
3. ‚úÖ Aggressive browser caching for static files
4. ‚úÖ Axios buffer cleanup interceptors
5. ‚úÖ Voice dialogue cleanup on disconnect
6. ‚úÖ GUN deletion infinite loop fix
7. ‚úÖ Circuit breaker in memory tracker
8. ‚úÖ Permanently failed records tracking in GUN sync
9. ‚úÖ Reduced logging verbosity across the app
10. ‚úÖ Identified Docker log tailing as major contributor

---

**Last Updated**: November 27, 2025
**Total Investigation Time**: ~6 hours across multiple sessions
**Memory Leak Reduction**: From ~500 MB/min to ~0 MB/min (when logs not tailed)
**Remaining Memory**: 5GB (down from 10GB), monitoring for further growth

