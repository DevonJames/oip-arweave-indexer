FROM nvidia/cuda:11.7.1-runtime-ubuntu20.04

# Set timezone to avoid interactive prompts
ENV DEBIAN_FRONTEND=noninteractive
ENV TZ=UTC

# Install system dependencies including FFmpeg development libraries
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libavcodec-dev \
    libavformat-dev \
    libavdevice-dev \
    libavutil-dev \
    libavfilter-dev \
    libswscale-dev \
    libswresample-dev \
    python3 \
    python3-pip \
    python3-dev \
    build-essential \
    pkg-config \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Install Python dependencies with GPU support
RUN pip3 install --upgrade pip

# Install PyTorch with CUDA support (cached layer)
RUN pip3 install --no-cache-dir \
    torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117

# Pre-install av separately with extended timeout (most problematic package)
# This creates a separate cached Docker layer
# Note: Using av>=11.0.0 for Cython 3.x compatibility (av 10.x has build issues)
RUN pip3 install --timeout=300 --retries=5 'av>=11.0.0,<13.0.0'

# Install stable dependencies (cached layer if av doesn't change)
RUN pip3 install --no-cache-dir \
    fastapi==0.104.1 \
    uvicorn==0.24.0 \
    python-multipart==0.0.6 \
    httpx==0.25.2 \
    pydantic==2.5.0 \
    numpy

# Pre-install tokenizers (prevents broken 0.21.x source build with puccinialin error)
RUN pip3 install --no-cache-dir 'tokenizers>=0.13,<0.20'

# Install ctranslate2 compatible with CUDA 11.7 (versions before 3.20.0 work without cuDNN 9)
RUN pip3 install --no-cache-dir ctranslate2==3.17.1

# Install faster-whisper compatible with ctranslate2 3.17.1
RUN pip3 install --no-cache-dir \
    --timeout=300 \
    --retries=5 \
    faster-whisper==0.9.0 \
    librosa

# Create models directory
RUN mkdir -p /app/models

# Copy service files
COPY . .

# Set environment variables for GPU
# Note: int8 on CUDA works without cuDNN (float16 requires cuDNN 9+ which needs CUDA 12+)
ENV CUDA_VISIBLE_DEVICES=0
ENV WHISPER_DEVICE=cuda
ENV WHISPER_COMPUTE_TYPE=int8

# Expose port
EXPOSE 8003

# Run the service
CMD ["uvicorn", "whisper_service:app", "--host", "0.0.0.0", "--port", "8003"] 