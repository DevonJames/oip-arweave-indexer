<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ALFRED - Mac Voice Interface</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            color: white;
        }

        .container {
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 8px 32px rgba(31, 38, 135, 0.37);
            border: 1px solid rgba(255, 255, 255, 0.18);
            max-width: 600px;
            width: 90%;
            text-align: center;
        }

        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            background: linear-gradient(45deg, #fff, #f0f0f0);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .subtitle {
            font-size: 1.1em;
            opacity: 0.8;
            margin-bottom: 40px;
        }

        .voice-button {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            border: none;
            background: linear-gradient(45deg, #4CAF50, #45a049);
            color: white;
            font-size: 2em;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
            margin: 20px;
            display: inline-flex;
            align-items: center;
            justify-content: center;
        }

        .voice-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(0, 0, 0, 0.3);
        }

        .voice-button.listening {
            background: linear-gradient(45deg, #2196F3, #1976D2);
            animation: pulse 2s infinite;
        }

        .voice-button.speaking {
            background: linear-gradient(45deg, #FF9800, #F57C00);
            animation: pulse 1s infinite;
        }

        .voice-button.processing {
            background: linear-gradient(45deg, #9C27B0, #7B1FA2);
            animation: spin 1s linear infinite;
        }

        .voice-button.disabled {
            background: linear-gradient(45deg, #757575, #616161);
            cursor: not-allowed;
            opacity: 0.6;
        }

        @keyframes pulse {
            0% { transform: scale(1); box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2); }
            50% { transform: scale(1.05); box-shadow: 0 6px 25px rgba(0, 0, 0, 0.4); }
            100% { transform: scale(1); box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2); }
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .status {
            margin: 20px 0;
            font-size: 1.1em;
            min-height: 25px;
        }

        .vad-indicator {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
            margin: 15px 0;
            font-size: 0.9em;
            opacity: 0.8;
        }

        .vad-bar {
            width: 200px;
            height: 6px;
            background: rgba(255, 255, 255, 0.2);
            border-radius: 3px;
            overflow: hidden;
        }

        .vad-level {
            height: 100%;
            background: linear-gradient(90deg, #4CAF50, #FF9800, #F44336);
            width: 0%;
            transition: width 0.1s ease;
        }

        .conversation {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            padding: 20px;
            margin: 20px 0;
            text-align: left;
            max-height: 400px;
            overflow-y: auto;
        }

        .message {
            margin: 15px 0;
            padding: 15px;
            border-radius: 10px;
            animation: fadeIn 0.3s ease;
        }

        .user-message {
            background: rgba(52, 152, 219, 0.3);
            text-align: right;
        }

        .assistant-message {
            background: rgba(46, 204, 113, 0.3);
            text-align: left;
        }

        .system-message {
            background: rgba(149, 165, 166, 0.3);
            text-align: center;
            font-style: italic;
        }

        .message-meta {
            font-size: 0.8em;
            opacity: 0.7;
            margin-bottom: 5px;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .controls {
            display: flex;
            gap: 10px;
            justify-content: center;
            margin: 20px 0;
            flex-wrap: wrap;
        }

        .control-button {
            padding: 10px 20px;
            border: none;
            border-radius: 25px;
            background: rgba(255, 255, 255, 0.2);
            color: white;
            cursor: pointer;
            transition: all 0.3s ease;
            font-size: 0.9em;
        }

        .control-button:hover {
            background: rgba(255, 255, 255, 0.3);
            transform: translateY(-1px);
        }

        .control-button.active {
            background: rgba(76, 175, 80, 0.6);
        }

        .health-status {
            position: fixed;
            top: 20px;
            right: 20px;
            padding: 15px;
            border-radius: 10px;
            font-size: 0.9em;
            max-width: 300px;
        }

        .health-healthy {
            background: rgba(46, 204, 113, 0.9);
        }

        .health-degraded {
            background: rgba(255, 193, 7, 0.9);
        }

        .health-unhealthy {
            background: rgba(231, 76, 60, 0.9);
        }

        .settings-panel {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            padding: 20px;
            margin: 20px 0;
            text-align: left;
            display: none;
        }

        .setting-group {
            margin: 15px 0;
        }

        .setting-label {
            display: block;
            margin-bottom: 5px;
            font-weight: bold;
        }

        .setting-input {
            width: 100%;
            padding: 8px;
            border: none;
            border-radius: 5px;
            background: rgba(255, 255, 255, 0.2);
            color: white;
        }

        .setting-input::placeholder {
            color: rgba(255, 255, 255, 0.6);
        }

        .debug-info {
            position: fixed;
            bottom: 20px;
            left: 20px;
            background: rgba(0, 0, 0, 0.8);
            color: #00ff00;
            padding: 10px;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            font-size: 0.8em;
            max-width: 400px;
            display: none;
        }
    </style>
</head>
<body>
    <div class="health-status" id="healthStatus">
        Checking services...
    </div>

    <div class="container">
        <h1>üçé ALFRED</h1>
        <p class="subtitle">Mac Voice Interface</p>
        
        <button class="voice-button" id="voiceButton">
            üé§
        </button>
        
        <div class="status" id="status">Ready! Click to start talking</div>
        
        <div class="vad-indicator" id="vadIndicator" style="display: none;">
            <span>üîä</span>
            <div class="vad-bar">
                <div class="vad-level" id="vadLevel"></div>
            </div>
            <span id="vadStatus">Listening...</span>
        </div>
        
        <div class="conversation" id="conversation" style="display: none;">
            <!-- Messages will appear here -->
        </div>
        
        <div class="controls">
            <button class="control-button" id="continuousBtn" onclick="toggleContinuousMode()">Continuous Mode</button>
            <button class="control-button" onclick="clearConversation()">Clear</button>
            <button class="control-button" onclick="checkHealth()">Health Check</button>
            <button class="control-button" onclick="toggleSettings()">Settings</button>
            <button class="control-button" onclick="toggleDebug()">Debug</button>
        </div>

        <div class="settings-panel" id="settingsPanel">
            <h3>Settings</h3>
            <div class="setting-group">
                <label class="setting-label">VAD Sensitivity</label>
                <input type="range" class="setting-input" id="vadThreshold" min="0.1" max="0.9" step="0.1" value="0.3">
            </div>
            <div class="setting-group">
                <label class="setting-label">Smart Turn Confidence</label>
                <input type="range" class="setting-input" id="smartTurnThreshold" min="0.2" max="0.8" step="0.05" value="0.35">
            </div>
            <div class="setting-group">
                <label class="setting-label">Language</label>
                <select id="language" class="setting-input">
                    <option value="en">English</option>
                    <option value="es">Spanish</option>
                    <option value="fr">French</option>
                    <option value="de">German</option>
                    <option value="it">Italian</option>
                    <option value="pt">Portuguese</option>
                    <option value="ja">Japanese</option>
                    <option value="ko">Korean</option>
                    <option value="zh">Chinese</option>
                </select>
            </div>
            <div class="setting-group">
                <label class="setting-label">Auto-play Responses</label>
                <input type="checkbox" id="autoPlayTTS" checked>
            </div>
        </div>
    </div>

    <div class="debug-info" id="debugInfo">
        Debug information will appear here...
    </div>

    <script>
        // State management
        let mediaRecorder;
        let audioChunks = [];
        let isListening = false;
        let isProcessing = false;
        let isSpeaking = false;
        let continuousMode = false;
        let vadAnalyzer;
        let audioContext;
        let analyserNode;
        let dataArray;
        let animationId;

        // Settings
        let settings = {
            vadThreshold: 0.3,           // Lower VAD threshold for better sensitivity
            smartTurnThreshold: 0.35,    // Lower Smart Turn threshold for better endpoint detection
            autoPlayTTS: true,
            language: 'en'               // Default language
        };

        // DOM elements
        const voiceButton = document.getElementById('voiceButton');
        const status = document.getElementById('status');
        const conversation = document.getElementById('conversation');
        const healthStatus = document.getElementById('healthStatus');
        const vadIndicator = document.getElementById('vadIndicator');
        const vadLevel = document.getElementById('vadLevel');
        const vadStatus = document.getElementById('vadStatus');
        const continuousBtn = document.getElementById('continuousBtn');
        const settingsPanel = document.getElementById('settingsPanel');
        const debugInfo = document.getElementById('debugInfo');

        // Initialize
        document.addEventListener('DOMContentLoaded', function() {
            checkHealth();
            setupAudio();
            loadSettings();
        });

        async function setupAudio() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });
                
                // Setup MediaRecorder - find the best supported format for real-time processing
                const preferredFormats = [
                    'audio/webm;codecs=pcm',      // PCM in WebM container (best for processing)
                    'audio/ogg;codecs=opus',      // Opus in OGG container
                    'audio/webm;codecs=opus',     // Opus in WebM container (fallback)
                    'audio/mp4',                  // MP4 container
                    'audio/mpeg'                  // MPEG format
                ];
                
                let mimeType = 'audio/webm;codecs=opus'; // Default fallback
                for (const format of preferredFormats) {
                    if (MediaRecorder.isTypeSupported(format)) {
                        mimeType = format;
                        break;
                    }
                }
                
                mediaRecorder = new MediaRecorder(stream, {
                    mimeType: mimeType,
                    audioBitsPerSecond: 16000 // Lower bitrate for faster processing
                });
                
                debug(`Using audio format: ${mimeType}`);
                
                mediaRecorder.ondataavailable = event => {
                    audioChunks.push(event.data);
                };
                
                mediaRecorder.onstop = processAudio;
                
                // Setup Web Audio API for VAD visualization
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const source = audioContext.createMediaStreamSource(stream);
                analyserNode = audioContext.createAnalyser();
                analyserNode.fftSize = 256;
                source.connect(analyserNode);
                
                dataArray = new Uint8Array(analyserNode.frequencyBinCount);
                
                status.textContent = 'Ready! Click to start talking';
                debug('Audio setup complete');
            } catch (error) {
                status.textContent = 'Microphone access denied. Please allow microphone access.';
                console.error('Audio setup error:', error);
                debug(`Audio setup error: ${error.message}`);
            }
        }

        voiceButton.addEventListener('click', handleVoiceButtonClick);

        function handleVoiceButtonClick() {
            if (isProcessing || isSpeaking) return;
            
            if (continuousMode) {
                toggleContinuousListening();
            } else {
                if (!isListening) {
                    startListening();
                } else {
                    stopListening();
                }
            }
        }

        function toggleContinuousMode() {
            continuousMode = !continuousMode;
            continuousBtn.classList.toggle('active', continuousMode);
            
            if (continuousMode) {
                continuousBtn.textContent = 'Stop Continuous';
                status.textContent = 'Continuous mode enabled - Click to start listening';
                debug('Continuous mode enabled');
            } else {
                continuousBtn.textContent = 'Continuous Mode';
                if (isListening) {
                    stopListening();
                }
                status.textContent = 'Continuous mode disabled';
                debug('Continuous mode disabled');
            }
        }

        function toggleContinuousListening() {
            if (isListening) {
                stopListening();
            } else {
                startContinuousListening();
            }
        }

        function startListening() {
            if (!mediaRecorder) {
                status.textContent = 'Microphone not available';
                return;
            }
            
            // CRITICAL: Always clear audio chunks when starting to listen
            audioChunks = [];
            debug(`Cleared audio chunks in startListening(), starting fresh`);
            
            mediaRecorder.start(100); // Collect data every 100ms for responsiveness
            isListening = true;
            
            voiceButton.classList.add('listening');
            voiceButton.textContent = 'üîµ';
            vadIndicator.style.display = 'flex';
            
            if (continuousMode) {
                status.textContent = 'Listening continuously... Smart Turn will detect endpoints';
                vadStatus.textContent = 'Continuous listening...';
            } else {
                status.textContent = 'Recording... Click to stop';
                vadStatus.textContent = 'Recording...';
            }
            
            startVADVisualization();
            debug('Started listening');
        }

        function startContinuousListening() {
            // CRITICAL: Clear audio chunks BEFORE starting to listen
            audioChunks = [];
            debug(`Cleared audio chunks before starting continuous listening`);
            
            startListening();
            
            // In continuous mode, we'll process audio in chunks and use Smart Turn to detect endpoints
            if (continuousMode) {
                // Double-check audio chunks are cleared
                audioChunks = [];
                debug(`Double-cleared audio chunks for continuous mode`);
                setTimeout(checkForEndpoint, 1000); // Start checking after 1 second for better responsiveness
            }
        }

        async function checkForEndpoint() {
            if (!isListening || !continuousMode) return;
            
            try {
                // Get current audio data
                if (audioChunks.length === 0) {
                    debug('No audio chunks available, waiting...');
                    setTimeout(checkForEndpoint, 1000); // Check less frequently when no audio
                    return;
                }
                
                debug(`Checking endpoint with ${audioChunks.length} audio chunks`);
                
                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                
                // Skip processing if audio is too short (likely just noise/fan)
                if (audioBlob.size < 8192) { // Less than ~8KB is probably just noise
                    debug(`Audio blob too small (${audioBlob.size} bytes), skipping - likely fan noise`);
                    audioChunks = [];
                    setTimeout(checkForEndpoint, 500);
                    return;
                }
                const formData = new FormData();
                formData.append('audio_file', audioBlob, 'recording.webm');
                
                // Quick STT to get transcript for Smart Turn
                const sttFormData = new FormData();
                sttFormData.append('file', audioBlob, 'recording.webm');
                sttFormData.append('use_vad', 'true');
                
                const sttResponse = await fetch('http://127.0.0.1:8013/transcribe_file', {
                    method: 'POST',
                    body: sttFormData
                });
                
                if (sttResponse.ok) {
                    const sttData = await sttResponse.json();
                    const transcript = sttData.text;
                    debug(`STT result: "${transcript}"`);
                    
                    if (transcript && transcript.trim().length > 0) {
                        // Check Smart Turn for endpoint detection
                        const smartTurnFormData = new FormData();
                        smartTurnFormData.append('audio_file', audioBlob, 'recording.webm');
                        smartTurnFormData.append('transcript', transcript);
                        
                        const smartTurnResponse = await fetch('http://127.0.0.1:8014/predict_endpoint', {
                            method: 'POST',
                            body: smartTurnFormData
                        });
                        
                        if (smartTurnResponse.ok) {
                            const smartTurnData = await smartTurnResponse.json();
                            debug(`Smart Turn: ${smartTurnData.prediction} (${smartTurnData.probability.toFixed(3)}) - threshold: ${settings.smartTurnThreshold}`);
                            
                            // If Smart Turn detects endpoint with high confidence, process the audio
                            if (smartTurnData.prediction === 1 && smartTurnData.probability >= settings.smartTurnThreshold) {
                                vadStatus.textContent = 'Endpoint detected!';
                                debug('Endpoint detected by Smart Turn, processing audio');
                                stopListening();
                                return;
                            } else {
                                vadStatus.textContent = `Listening... (confidence: ${smartTurnData.probability.toFixed(2)})`;
                                // Clear processed audio chunks to prevent reprocessing the same audio
                                debug(`Clearing ${audioChunks.length} audio chunks to prevent reprocessing`);
                                audioChunks = [];
                            }
                        } else {
                            debug(`Smart Turn request failed: ${smartTurnResponse.status}`);
                        }
                    } else {
                        debug('No transcript from STT, continuing...');
                        // Clear audio chunks when no transcript to prevent accumulation
                        audioChunks = [];
                    }
                } else {
                    debug(`STT request failed: ${sttResponse.status}`);
                    // Clear audio chunks when STT fails to prevent accumulation
                    audioChunks = [];
                }
                
                // Continue listening if no endpoint detected
                setTimeout(checkForEndpoint, 300); // Check every 0.3 seconds for better responsiveness
                
            } catch (error) {
                debug(`Endpoint check error: ${error.message}`);
                // Clear audio chunks on error to prevent accumulation
                audioChunks = [];
                setTimeout(checkForEndpoint, 1000);
            }
        }

        function stopListening() {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                debug(`Stopping MediaRecorder, current chunks: ${audioChunks.length}`);
                mediaRecorder.stop();
                isListening = false;
                
                voiceButton.classList.remove('listening');
                voiceButton.classList.add('processing');
                voiceButton.textContent = '‚è≥';
                status.textContent = 'Processing audio...';
                vadStatus.textContent = 'Processing...';
                
                stopVADVisualization();
                debug('Stopped listening, processing audio');
            } else {
                debug(`MediaRecorder state: ${mediaRecorder ? mediaRecorder.state : 'null'}`);
                // AGGRESSIVE FIX: Clear audio chunks even if MediaRecorder isn't recording
                audioChunks = [];
                debug(`Cleared audio chunks in stopListening() fallback`);
            }
        }

        function startVADVisualization() {
            if (!analyserNode) return;
            
            const updateVAD = () => {
                if (!isListening) return;
                
                analyserNode.getByteFrequencyData(dataArray);
                
                // Calculate RMS (root mean square) for volume level
                let sum = 0;
                for (let i = 0; i < dataArray.length; i++) {
                    sum += dataArray[i] * dataArray[i];
                }
                const rms = Math.sqrt(sum / dataArray.length);
                const normalizedLevel = Math.min(rms / 128, 1);
                
                vadLevel.style.width = `${normalizedLevel * 100}%`;
                
                animationId = requestAnimationFrame(updateVAD);
            };
            
            updateVAD();
        }

        function stopVADVisualization() {
            if (animationId) {
                cancelAnimationFrame(animationId);
                animationId = null;
            }
            vadLevel.style.width = '0%';
            vadIndicator.style.display = 'none';
        }

        async function processAudio() {
            try {
                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                
                if (audioBlob.size === 0) {
                    status.textContent = 'No audio detected. Try again.';
                    resetButton();
                    return;
                }

                // Step 1: Speech-to-Text with VAD
                status.textContent = 'Converting speech to text...';
                debug('Starting STT processing');
                
                const formData = new FormData();
                formData.append('file', audioBlob, 'recording.webm');
                formData.append('use_vad', 'true');
                formData.append('language', settings.language);

                const sttResponse = await fetch('http://127.0.0.1:8013/transcribe_file', {
                    method: 'POST',
                    body: formData
                });

                if (!sttResponse.ok) {
                    throw new Error(`STT failed: ${sttResponse.status}`);
                }

                const sttData = await sttResponse.json();
                const transcript = sttData.text;
                
                debug(`STT result: "${transcript}"`);
                
                if (!transcript || transcript.trim().length === 0) {
                    status.textContent = 'No speech detected. Try again.';
                    resetButton();
                    
                    if (continuousMode) {
                        setTimeout(() => startContinuousListening(), 1000);
                    }
                    return;
                }

                addMessage('user', transcript);
                
                // Step 2: Smart Turn endpoint detection (if not already processed in continuous mode)
                if (!continuousMode) {
                    status.textContent = 'Checking if complete...';
                    debug('Running Smart Turn endpoint detection');
                    
                    const smartTurnFormData = new FormData();
                    smartTurnFormData.append('audio_file', audioBlob, 'recording.webm');
                    smartTurnFormData.append('transcript', transcript);
                    
                    const smartTurnResponse = await fetch('http://127.0.0.1:8014/predict_endpoint', {
                        method: 'POST',
                        body: smartTurnFormData
                    });
                    
                    if (smartTurnResponse.ok) {
                        const smartTurnData = await smartTurnResponse.json();
                        debug(`Smart Turn: ${smartTurnData.prediction} (${smartTurnData.probability.toFixed(3)})`);
                        
                        if (smartTurnData.prediction === 0 || smartTurnData.probability < settings.smartTurnThreshold) {
                            addMessage('system', `Incomplete utterance detected (confidence: ${smartTurnData.probability.toFixed(2)}). Continue speaking or click to process anyway.`);
                            status.textContent = 'Continue speaking or click to send...';
                            resetButton();
                            return;
                        }
                    }
                }

                // Step 3: Send to backend for RAG processing
                status.textContent = 'Sending to ALFRED...';
                debug('Sending to backend for RAG processing');

                // First, initiate streaming conversation
                const initResponse = await fetch('/api/backend/voice/converse', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        userInput: transcript,
                        model: "llama3.2:3b",
                        conversationHistory: getConversationHistory(),
                        systemPrompt: "You are ALFRED (Autonomous Linguistic Framework for Retrieval & Enhanced Dialogue), a versatile and articulate AI assistant. You help by answering questions and retrieving information from stored records. You prioritize clarity, speed, and relevance. IMPORTANT: Do not use emojis, asterisks, or other markdown formatting in your responses, as they interfere with text-to-speech synthesis."
                    })
                });

                if (!initResponse.ok) {
                    throw new Error(`Backend failed: ${initResponse.status}`);
                }

                const initData = await initResponse.json();
                const dialogueId = initData.dialogueId;
                debug(`Started streaming conversation: ${dialogueId}`);

                // Step 4: Connect to real-time streaming response
                let fullResponse = '';
                let assistantMessageElement = null;

                const eventSource = new EventSource(`/api/backend/generate/open-stream?id=${dialogueId}`);
                
                eventSource.onopen = function() {
                    debug('Real-time streaming connection opened');
                    status.textContent = 'Receiving live response...';
                };

                eventSource.addEventListener('textChunk', function(event) {
                    const data = JSON.parse(event.data);
                    if (data.role === 'assistant' && data.text) {
                        fullResponse += data.text;
                        
                        // Create or update assistant message in real-time
                        if (!assistantMessageElement) {
                            assistantMessageElement = addMessage('assistant', fullResponse);
                        } else {
                            updateMessage(assistantMessageElement, fullResponse);
                        }
                        
                        debug(`Live text chunk: "${data.text}" (total: ${fullResponse.length} chars)`);
                    }
                });

                eventSource.addEventListener('audioChunk', function(event) {
                    const data = JSON.parse(event.data);
                    if (settings.autoPlayTTS && data.audio_data) {
                        // Play audio chunk immediately for real-time speech
                        debug('Received live audio chunk');
                        playAudioChunk(data.audio_data);
                    }
                });

                eventSource.addEventListener('complete', function(event) {
                    debug('Real-time streaming complete');
                    eventSource.close();
                    status.textContent = 'Live response complete';
                    
                    // Update conversation history with final response
                    if (fullResponse) {
                        addToConversationHistory('assistant', fullResponse);
                    }
                });

                eventSource.onerror = function(event) {
                    console.error('Real-time streaming error:', event);
                    eventSource.close();
                    if (!fullResponse) {
                        // Fallback if streaming failed
                        addMessage('assistant', 'I apologize, but I encountered an error while generating a response. Please try again.');
                    }
                    status.textContent = 'Ready';
                };

                status.textContent = continuousMode ? 'Listening continuously...' : 'Ready! Click to start talking';
                debug('Audio processing completed successfully');
                
            } catch (error) {
                console.error('Processing error:', error);
                status.textContent = `Error: ${error.message}`;
                addMessage('system', `Error: ${error.message}`);
                debug(`Processing error: ${error.message}`);
            } finally {
                resetButton();
                
                // In continuous mode, start listening again after a brief pause
                if (continuousMode && !isSpeaking) {
                    // CRITICAL: Clear audio chunks before restarting continuous mode
                    audioChunks = [];
                    debug(`Cleared audio chunks before restarting continuous mode`);
                    
                    // AGGRESSIVE FIX: Stop MediaRecorder completely and restart fresh
                    if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                        debug(`Force stopping MediaRecorder before restart`);
                        mediaRecorder.stop();
                    }
                    
                    setTimeout(() => {
                        // Double-clear before restart
                        audioChunks = [];
                        debug(`Double-cleared audio chunks before timeout restart`);
                        startContinuousListening();
                    }, 2000);
                }
            }
        }

        function resetButton() {
            isProcessing = false;
            isListening = false;
            voiceButton.classList.remove('listening', 'processing', 'speaking');
            voiceButton.textContent = 'üé§';
            
            if (continuousMode && !isSpeaking) {
                voiceButton.textContent = 'üîÑ';
            }
        }

        function addMessage(type, text) {
            conversation.style.display = 'block';
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${type}-message`;
            
            const timestamp = new Date().toLocaleTimeString();
            const prefix = type === 'user' ? 'You' : type === 'assistant' ? 'ALFRED' : 'System';
            
            messageDiv.innerHTML = `
                <div class="message-meta">${prefix} - ${timestamp}</div>
                <div class="message-content">${text}</div>
            `;
            
            conversation.appendChild(messageDiv);
            conversation.scrollTop = conversation.scrollHeight;
            
            return messageDiv; // Return the element so we can update it later
        }

        function updateMessage(messageElement, newText) {
            if (messageElement) {
                const contentElement = messageElement.querySelector('.message-content');
                if (contentElement) {
                    contentElement.textContent = newText;
                    conversation.scrollTop = conversation.scrollHeight;
                }
            }
        }

        async function playAudio(audioUrl) {
            try {
                const audio = new Audio(audioUrl);
                return new Promise((resolve, reject) => {
                    audio.onended = () => {
                        debug('Audio playback completed');
                        resolve();
                    };
                    audio.onerror = (error) => {
                        debug(`Audio playback error: ${error}`);
                        reject(error);
                    };
                    audio.play();
                });
            } catch (error) {
                debug(`Audio playback error: ${error.message}`);
                console.error('Audio playback error:', error);
            }
        }

        async function checkHealth() {
            try {
                const response = await fetch('/health');
                const data = await response.json();
                
                const sttHealthy = data.services?.stt?.status === 'healthy';
                const smartTurnHealthy = data.services?.smart_turn?.status === 'healthy';
                const backendHealthy = data.services?.backend?.status === 'healthy';
                
                const healthyCount = [sttHealthy, smartTurnHealthy, backendHealthy].filter(Boolean).length;
                const totalServices = 3;
                
                let statusClass, statusText;
                if (healthyCount === totalServices) {
                    statusClass = 'health-healthy';
                    statusText = '‚úÖ All Services Ready';
                } else if (healthyCount > 0) {
                    statusClass = 'health-degraded';
                    statusText = `‚ö†Ô∏è ${healthyCount}/${totalServices} Services Ready`;
                } else {
                    statusClass = 'health-unhealthy';
                    statusText = '‚ùå Services Unavailable';
                }
                
                healthStatus.className = `health-status ${statusClass}`;
                healthStatus.innerHTML = `
                    <div>${statusText}</div>
                    <div style="font-size: 0.8em; margin-top: 5px;">
                        STT: ${sttHealthy ? '‚úÖ' : '‚ùå'} | 
                        Smart Turn: ${smartTurnHealthy ? '‚úÖ' : '‚ùå'} | 
                        Backend: ${backendHealthy ? '‚úÖ' : '‚ùå'}
                    </div>
                `;
                
                if (healthyCount < totalServices) {
                    debug(`Health issues: STT=${sttHealthy}, SmartTurn=${smartTurnHealthy}, Backend=${backendHealthy}`);
                }
                
                // Disable voice button if critical services are down
                if (!sttHealthy || !smartTurnHealthy) {
                    voiceButton.classList.add('disabled');
                    voiceButton.style.pointerEvents = 'none';
                } else {
                    voiceButton.classList.remove('disabled');
                    voiceButton.style.pointerEvents = 'auto';
                }
                
            } catch (error) {
                healthStatus.className = 'health-status health-unhealthy';
                healthStatus.textContent = '‚ùå Connection Error';
                debug(`Health check error: ${error.message}`);
                console.error('Health check error:', error);
            }
        }

        function clearConversation() {
            conversation.innerHTML = '';
            conversation.style.display = 'none';
            debug('Conversation cleared');
        }

        function toggleSettings() {
            const isVisible = settingsPanel.style.display === 'block';
            settingsPanel.style.display = isVisible ? 'none' : 'block';
            
            if (!isVisible) {
                loadSettings();
            } else {
                saveSettings();
            }
        }

        function loadSettings() {
            const saved = localStorage.getItem('alfred-voice-settings');
            if (saved) {
                settings = { ...settings, ...JSON.parse(saved) };
            }
            
            document.getElementById('vadThreshold').value = settings.vadThreshold;
            document.getElementById('smartTurnThreshold').value = settings.smartTurnThreshold;
            document.getElementById('autoPlayTTS').checked = settings.autoPlayTTS;
            document.getElementById('language').value = settings.language;
        }

        function saveSettings() {
            settings.vadThreshold = parseFloat(document.getElementById('vadThreshold').value);
            settings.smartTurnThreshold = parseFloat(document.getElementById('smartTurnThreshold').value);
            settings.autoPlayTTS = document.getElementById('autoPlayTTS').checked;
            settings.language = document.getElementById('language').value;
            
            localStorage.setItem('alfred-voice-settings', JSON.stringify(settings));
            debug('Settings saved');
        }

        function toggleDebug() {
            const isVisible = debugInfo.style.display === 'block';
            debugInfo.style.display = isVisible ? 'none' : 'block';
        }

        function debug(message) {
            const timestamp = new Date().toLocaleTimeString();
            const debugMessage = `[${timestamp}] ${message}\n`;
            
            debugInfo.textContent = debugMessage + debugInfo.textContent;
            
            // Keep only last 20 lines
            const lines = debugInfo.textContent.split('\n');
            if (lines.length > 20) {
                debugInfo.textContent = lines.slice(0, 20).join('\n');
            }
            
            console.log(`[DEBUG] ${message}`);
        }

        // Auto health check every 30 seconds
        setInterval(checkHealth, 30000);

        // Auto-save settings when changed
        document.addEventListener('change', function(e) {
            if (e.target.closest('#settingsPanel')) {
                saveSettings();
                debug('Settings updated');
            }
        });

        // Get conversation history for ALFRED context
        function getConversationHistory() {
            const messages = document.querySelectorAll('.message');
            const history = [];
            
            messages.forEach(msg => {
                const sender = msg.querySelector('.sender')?.textContent?.toLowerCase();
                const content = msg.querySelector('.message-content')?.textContent;
                
                if (sender === 'you' && content) {
                    history.push({ role: 'user', content: content });
                } else if (sender === 'alfred' && content) {
                    history.push({ role: 'assistant', content: content });
                }
            });
            
            // Return last 6 messages to keep context manageable
            return history.slice(-6);
        }

        // Add to conversation history (for streaming updates)
        function addToConversationHistory(role, content) {
            // This is handled by the streaming UI updates
            debug(`Added to conversation: ${role} - ${content.substring(0, 50)}...`);
        }

        // Play audio chunk for real-time streaming audio
        function playAudioChunk(audioData) {
            if (!audioData) return;
            
            try {
                const audioBlob = new Blob([Uint8Array.from(atob(audioData), c => c.charCodeAt(0))], {
                    type: 'audio/wav'
                });
                const audioUrl = URL.createObjectURL(audioBlob);
                const audio = new Audio(audioUrl);
                
                audio.play().catch(error => {
                    console.error('Audio chunk playback failed:', error);
                });
                
                // Clean up URL after playback
                audio.addEventListener('ended', () => {
                    URL.revokeObjectURL(audioUrl);
                });
            } catch (error) {
                console.error('Error playing audio chunk:', error);
            }
        }

        // Keyboard shortcuts
        document.addEventListener('keydown', function(e) {
            if (e.code === 'Space' && !e.target.matches('input, textarea')) {
                e.preventDefault();
                handleVoiceButtonClick();
            }
            
            if (e.key === 'Escape' && isListening) {
                stopListening();
            }
            
            if (e.key === 'c' && e.ctrlKey) {
                clearConversation();
            }
        });
    </script>
</body>
</html>